---
phase: 04-data-ingestion-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - apps/omnii_mcp/src/ingestion/sync-state.ts
  - supabase/migrations/20260125_sync_state.sql
autonomous: true

must_haves:
  truths:
    - "Sync state can be saved with sync token for a user+source combination"
    - "Sync state can be retrieved to resume incremental sync"
    - "Last sync timestamp is tracked for each user+source"
    - "Sync token expiry (410 errors) triggers full sync by clearing token"
  artifacts:
    - path: "apps/omnii_mcp/src/ingestion/sync-state.ts"
      provides: "CRUD operations for sync state persistence"
      exports: ["SyncStateService", "SyncSource"]
    - path: "supabase/migrations/20260125_sync_state.sql"
      provides: "Supabase table for sync state storage"
      contains: "CREATE TABLE sync_state"
  key_links:
    - from: "apps/omnii_mcp/src/ingestion/sync-state.ts"
      to: "@supabase/supabase-js"
      via: "Supabase client for persistence"
      pattern: "createClient|supabase"
---

<objective>
Create sync state persistence layer for incremental sync token/historyId tracking

Purpose: Enable delta updates instead of full refresh - store syncToken (Calendar/Contacts), historyId (Gmail), updatedMin (Tasks) per user
Output: SyncStateService with Supabase-backed persistence and SQL migration
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-data-ingestion-pipeline/04-RESEARCH.md
@apps/omnii_mcp/src/config/env.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Supabase migration for sync_state table</name>
  <files>supabase/migrations/20260125_sync_state.sql</files>
  <action>
Create SQL migration for sync state storage table.

Create `supabase/migrations/20260125_sync_state.sql`:

```sql
-- Sync State Table for Incremental Data Ingestion
--
-- Tracks sync tokens, history IDs, and timestamps for each user+source combination.
-- Enables delta updates instead of full refresh for Google service sync.

CREATE TYPE sync_source AS ENUM (
  'google_calendar',
  'google_tasks',
  'google_gmail',
  'google_contacts'
);

CREATE TYPE sync_status AS ENUM (
  'idle',
  'syncing',
  'error',
  'rate_limited'
);

CREATE TABLE IF NOT EXISTS sync_state (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  source sync_source NOT NULL,

  -- Sync token fields (different services use different mechanisms)
  sync_token TEXT, -- Calendar, Contacts: syncToken from API response
  history_id TEXT, -- Gmail: historyId for incremental history fetch
  updated_min TIMESTAMPTZ, -- Tasks: updatedMin timestamp for filtering

  -- Tracking fields
  last_sync_at TIMESTAMPTZ,
  last_successful_sync_at TIMESTAMPTZ,
  status sync_status DEFAULT 'idle',
  error_message TEXT,
  items_synced INTEGER DEFAULT 0,

  -- Timestamps
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),

  -- Unique constraint: one sync state per user per source
  UNIQUE(user_id, source)
);

-- Index for fast lookup by user
CREATE INDEX idx_sync_state_user_id ON sync_state(user_id);

-- Index for finding stale syncs (background job scheduling)
CREATE INDEX idx_sync_state_last_sync ON sync_state(last_sync_at);

-- Index for finding syncs by status (error recovery)
CREATE INDEX idx_sync_state_status ON sync_state(status);

-- Auto-update updated_at timestamp
CREATE OR REPLACE FUNCTION update_sync_state_timestamp()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER sync_state_updated_at
  BEFORE UPDATE ON sync_state
  FOR EACH ROW
  EXECUTE FUNCTION update_sync_state_timestamp();

-- Row Level Security
ALTER TABLE sync_state ENABLE ROW LEVEL SECURITY;

-- Users can only access their own sync state
CREATE POLICY "Users can view own sync state"
  ON sync_state FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own sync state"
  ON sync_state FOR INSERT
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own sync state"
  ON sync_state FOR UPDATE
  USING (auth.uid() = user_id);

-- Service role can access all (for background jobs)
CREATE POLICY "Service role full access"
  ON sync_state FOR ALL
  USING (auth.jwt() ->> 'role' = 'service_role');

COMMENT ON TABLE sync_state IS 'Tracks incremental sync state for Google service data ingestion';
COMMENT ON COLUMN sync_state.sync_token IS 'Calendar/Contacts: syncToken from API for delta updates';
COMMENT ON COLUMN sync_state.history_id IS 'Gmail: historyId for incremental history fetch';
COMMENT ON COLUMN sync_state.updated_min IS 'Tasks: updatedMin timestamp for filtering changed tasks';
```

Note: This migration should be applied via Supabase Dashboard or CLI. The file is placed in supabase/migrations/ following Supabase convention.
  </action>
  <verify>
File exists at supabase/migrations/20260125_sync_state.sql.
SQL is syntactically valid (no obvious errors).
  </verify>
  <done>SQL migration creates sync_state table with RLS policies</done>
</task>

<task type="auto">
  <name>Task 2: Create SyncStateService for persistence operations</name>
  <files>apps/omnii_mcp/src/ingestion/sync-state.ts</files>
  <action>
Create SyncStateService class for sync state CRUD operations.

Create `apps/omnii_mcp/src/ingestion/sync-state.ts`:

```typescript
/**
 * Sync State Service
 *
 * Manages persistence of sync tokens and history IDs for incremental sync.
 * Uses Supabase for storage with per-user isolation via RLS.
 */

import { createClient, SupabaseClient } from "@supabase/supabase-js";

/**
 * Sync sources matching the database enum
 */
export type SyncSource =
  | "google_calendar"
  | "google_tasks"
  | "google_gmail"
  | "google_contacts";

/**
 * Sync status matching the database enum
 */
export type SyncStatus = "idle" | "syncing" | "error" | "rate_limited";

/**
 * Sync state record structure
 */
export interface SyncState {
  id: string;
  user_id: string;
  source: SyncSource;
  sync_token: string | null;
  history_id: string | null;
  updated_min: string | null;
  last_sync_at: string | null;
  last_successful_sync_at: string | null;
  status: SyncStatus;
  error_message: string | null;
  items_synced: number;
  created_at: string;
  updated_at: string;
}

/**
 * Input for creating/updating sync state
 */
export interface SyncStateUpdate {
  sync_token?: string | null;
  history_id?: string | null;
  updated_min?: string | null;
  status?: SyncStatus;
  error_message?: string | null;
  items_synced?: number;
  last_sync_at?: string;
  last_successful_sync_at?: string;
}

/**
 * SyncStateService manages sync token persistence for incremental sync.
 *
 * Usage:
 * ```typescript
 * const syncState = new SyncStateService();
 *
 * // Get current sync state for user
 * const state = await syncState.getState(userId, "google_calendar");
 *
 * // Update after successful sync
 * await syncState.updateState(userId, "google_calendar", {
 *   sync_token: nextSyncToken,
 *   status: "idle",
 *   last_successful_sync_at: new Date().toISOString(),
 * });
 *
 * // Clear token on 410 error (expired)
 * await syncState.clearSyncToken(userId, "google_calendar");
 * ```
 */
export class SyncStateService {
  private supabase: SupabaseClient;

  constructor() {
    const url = process.env.OMNII_SUPABASE_URL;
    const key = process.env.OMNII_SUPABASE_SERVICE_ROLE_KEY;

    if (!url || !key) {
      throw new Error(
        "OMNII_SUPABASE_URL and OMNII_SUPABASE_SERVICE_ROLE_KEY required for SyncStateService"
      );
    }

    // Use service role key for background job access (bypasses RLS)
    this.supabase = createClient(url, key);
  }

  /**
   * Get sync state for a user and source.
   * Returns null if no sync has occurred yet.
   */
  async getState(
    userId: string,
    source: SyncSource
  ): Promise<SyncState | null> {
    const { data, error } = await this.supabase
      .from("sync_state")
      .select("*")
      .eq("user_id", userId)
      .eq("source", source)
      .single();

    if (error && error.code !== "PGRST116") {
      // PGRST116 = no rows returned
      throw new Error(`Failed to get sync state: ${error.message}`);
    }

    return data as SyncState | null;
  }

  /**
   * Create or update sync state for a user and source.
   * Uses upsert to handle both create and update in one call.
   */
  async updateState(
    userId: string,
    source: SyncSource,
    updates: SyncStateUpdate
  ): Promise<SyncState> {
    const { data, error } = await this.supabase
      .from("sync_state")
      .upsert(
        {
          user_id: userId,
          source,
          ...updates,
        },
        {
          onConflict: "user_id,source",
        }
      )
      .select()
      .single();

    if (error) {
      throw new Error(`Failed to update sync state: ${error.message}`);
    }

    return data as SyncState;
  }

  /**
   * Mark sync as started.
   * Sets status to "syncing" and updates last_sync_at.
   */
  async markSyncStarted(userId: string, source: SyncSource): Promise<void> {
    await this.updateState(userId, source, {
      status: "syncing",
      last_sync_at: new Date().toISOString(),
      error_message: null,
    });
  }

  /**
   * Mark sync as completed successfully.
   * Updates sync token and status.
   */
  async markSyncCompleted(
    userId: string,
    source: SyncSource,
    syncData: {
      syncToken?: string;
      historyId?: string;
      updatedMin?: string;
      itemsSynced: number;
    }
  ): Promise<void> {
    await this.updateState(userId, source, {
      sync_token: syncData.syncToken ?? null,
      history_id: syncData.historyId ?? null,
      updated_min: syncData.updatedMin ?? null,
      status: "idle",
      items_synced: syncData.itemsSynced,
      last_successful_sync_at: new Date().toISOString(),
    });
  }

  /**
   * Mark sync as failed with error.
   */
  async markSyncFailed(
    userId: string,
    source: SyncSource,
    errorMessage: string
  ): Promise<void> {
    await this.updateState(userId, source, {
      status: "error",
      error_message: errorMessage,
    });
  }

  /**
   * Mark sync as rate limited.
   * Background job should wait before retrying.
   */
  async markRateLimited(userId: string, source: SyncSource): Promise<void> {
    await this.updateState(userId, source, {
      status: "rate_limited",
      error_message: "API rate limit exceeded",
    });
  }

  /**
   * Clear sync token to trigger full sync.
   * Used when receiving 410 Gone (Calendar/Contacts) or 404 (Gmail historyId expired).
   */
  async clearSyncToken(userId: string, source: SyncSource): Promise<void> {
    await this.updateState(userId, source, {
      sync_token: null,
      history_id: null,
      updated_min: null,
    });
  }

  /**
   * Get all users with a specific source that need sync.
   * Used by background job scheduler to batch sync operations.
   */
  async getUsersNeedingSync(
    source: SyncSource,
    staleMinutes: number = 15
  ): Promise<string[]> {
    const staleTime = new Date(Date.now() - staleMinutes * 60 * 1000);

    const { data, error } = await this.supabase
      .from("sync_state")
      .select("user_id")
      .eq("source", source)
      .eq("status", "idle")
      .or(`last_sync_at.is.null,last_sync_at.lt.${staleTime.toISOString()}`);

    if (error) {
      throw new Error(`Failed to get users needing sync: ${error.message}`);
    }

    return (data ?? []).map((row) => row.user_id);
  }
}

// Singleton instance
let _syncStateService: SyncStateService | null = null;

/**
 * Get singleton SyncStateService instance.
 */
export function getSyncStateService(): SyncStateService {
  if (!_syncStateService) {
    _syncStateService = new SyncStateService();
  }
  return _syncStateService;
}
```
  </action>
  <verify>
File exists at apps/omnii_mcp/src/ingestion/sync-state.ts.
TypeScript compiles: `cd apps/omnii_mcp && bunx tsc --noEmit src/ingestion/sync-state.ts`
  </verify>
  <done>SyncStateService provides CRUD operations for sync state with Supabase persistence</done>
</task>

<task type="auto">
  <name>Task 3: Update ingestion barrel export</name>
  <files>apps/omnii_mcp/src/ingestion/index.ts</files>
  <action>
Update the ingestion barrel export to include sync state service.

Update `apps/omnii_mcp/src/ingestion/index.ts`:

```typescript
/**
 * Data Ingestion Module
 *
 * Infrastructure for ingesting external data sources into the knowledge graph.
 * - Composio client for Google OAuth and API calls
 * - BullMQ queue for background job processing
 * - Sync state for incremental sync tracking
 * - Validators for data quality gates
 */

// Infrastructure
export { getComposioClient, type ComposioClient } from "./composio-client";
export { getRedisConnection, createIngestionQueue } from "./jobs/queue";

// Sync State
export {
  SyncStateService,
  getSyncStateService,
  type SyncSource,
  type SyncStatus,
  type SyncState,
  type SyncStateUpdate,
} from "./sync-state";

// Validators (will be added after Plan 02)
// export * from "./validators";
```
  </action>
  <verify>
TypeScript compiles: `cd apps/omnii_mcp && bunx tsc --noEmit src/ingestion/index.ts`
  </verify>
  <done>Ingestion module exports sync state service</done>
</task>

</tasks>

<verification>
1. Migration file exists with correct schema
2. SyncStateService compiles: `cd apps/omnii_mcp && bunx tsc --noEmit src/ingestion/sync-state.ts`
3. Barrel export includes sync state: Check exports in index.ts
</verification>

<success_criteria>
- SQL migration creates sync_state table with proper types and RLS
- SyncStateService provides getState, updateState, clearSyncToken operations
- Helper methods exist for common operations (markSyncStarted, markSyncCompleted, markSyncFailed)
- getUsersNeedingSync enables background job scheduling
- Singleton pattern provides reusable service instance
- All files compile without TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-data-ingestion-pipeline/04-03-SUMMARY.md`
</output>
