---
phase: 03-graphrag-advanced-mcp
plan: 06
type: execute
wave: 4
depends_on: ["03-05"]
files_modified:
  - apps/omnii_mcp/src/mcp/adapters/local-llm.ts
  - apps/omnii_mcp/src/mcp/adapters/index.ts
  - apps/omnii_mcp/src/routes/local-llm.ts
  - apps/omnii_mcp/src/app.ts
autonomous: true

must_haves:
  truths:
    - "Local LLM (Ollama) can request tool execution via HTTP endpoint"
    - "MCP tools converted to Ollama-compatible format"
    - "Tool results returned in format local LLMs expect"
    - "Graceful handling of models without tool calling support"
  artifacts:
    - path: "apps/omnii_mcp/src/mcp/adapters/local-llm.ts"
      provides: "Local LLM adapter for MCP tools (Ollama/LM Studio)"
      exports: ["convertMCPToolToOllama", "handleLocalLLMToolCalls", "LocalLLMToolAdapter"]
    - path: "apps/omnii_mcp/src/routes/local-llm.ts"
      provides: "HTTP endpoint for local LLM tool execution"
      exports: ["localLLMRoutes"]
  key_links:
    - from: "adapters/local-llm.ts"
      to: "mcp/tools/index.ts"
      via: "TOOL_DEFINITIONS, TOOL_HANDLERS"
      pattern: "TOOL_DEFINITIONS"
    - from: "adapters/local-llm.ts"
      to: "adapters/openai.ts"
      via: "Similar pattern, Ollama-specific format"
      pattern: "convertMCPTool"
---

<objective>
Implement local LLM bridge for MCP tool invocation.

Purpose: Enable privacy-focused deployments where users run local LLMs (Ollama, LM Studio) that can invoke MCP tools without sending data to cloud APIs. This is a LOWER priority than OpenAI (per research), implementing a basic bridge that can be extended later.

Output: Local LLM adapter with Ollama-compatible tool format and HTTP endpoints
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-graphrag-advanced-mcp/03-RESEARCH.md

# OpenAI adapter pattern to follow
@apps/omnii_mcp/src/mcp/adapters/openai.ts

# MCP tools to expose
@apps/omnii_mcp/src/mcp/tools/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create local LLM tool adapter</name>
  <files>
    apps/omnii_mcp/src/mcp/adapters/local-llm.ts
  </files>
  <action>
Create `apps/omnii_mcp/src/mcp/adapters/local-llm.ts`:

1. **Type definitions** (Ollama format - similar to OpenAI but with differences):
   ```typescript
   export interface OllamaTool {
     type: 'function';
     function: {
       name: string;
       description: string;
       parameters: {
         type: 'object';
         properties: Record<string, unknown>;
         required?: string[];
       };
     };
   }

   export interface OllamaToolCall {
     function: {
       name: string;
       arguments: Record<string, unknown>;  // Parsed object, not string
     };
   }

   export interface LocalLLMToolResult {
     role: 'tool';
     content: string;
   }

   // For LM Studio compatibility (uses OpenAI format)
   export type LMStudioTool = OpenAITool;  // Same as OpenAI
   ```

2. **convertMCPToolToOllama function**:
   ```typescript
   export function convertMCPToolToOllama(mcpTool: MCPToolDefinition): OllamaTool
   ```
   - Similar to OpenAI conversion but without strict: true (Ollama doesn't support it)
   - Note: LM Studio uses OpenAI format, so reuse convertMCPToolToOpenAI for it

3. **getAllToolsForOllama function**:
   ```typescript
   export function getAllToolsForOllama(): OllamaTool[]
   ```

4. **handleLocalLLMToolCalls function**:
   ```typescript
   export async function handleLocalLLMToolCalls(
     toolCalls: OllamaToolCall[],
     context: { userId: string; client: Neo4jClient }
   ): Promise<LocalLLMToolResult[]>
   ```
   - IMPORTANT: Process sequentially (not parallel) - local LLMs often unreliable with parallel calls per research
   - For each tool call:
     - Arguments already parsed (Ollama returns object, not string)
     - Get handler from TOOL_HANDLERS
     - Execute handler
     - Convert to local LLM result format
   - Include extra error handling for tool call hallucinations (local LLMs more prone)

5. **validateToolCall helper** (internal):
   ```typescript
   function validateToolCall(toolCall: OllamaToolCall): boolean
   ```
   - Check if tool name exists in TOOL_HANDLERS
   - Validate arguments have expected types
   - Return false for hallucinated/invalid tool calls

6. **LocalLLMToolAdapter class**:
   ```typescript
   export class LocalLLMToolAdapter {
     constructor(private context: { userId: string; client: Neo4jClient }) {}

     getTools(format: 'ollama' | 'lmstudio' = 'ollama'): OllamaTool[] | OpenAITool[]
     async executeToolCalls(
       toolCalls: OllamaToolCall[],
       options?: { sequential?: boolean }  // Default: true for local LLMs
     ): Promise<LocalLLMToolResult[]>
   }
   ```
  </action>
  <verify>
Run TypeScript compilation: `cd apps/omnii_mcp && bunx tsc --noEmit`

Check exports: `grep "export" apps/omnii_mcp/src/mcp/adapters/local-llm.ts`

Verify sequential execution: `grep -E "for.*of|forEach|await.*toolCall" apps/omnii_mcp/src/mcp/adapters/local-llm.ts` (should use for-of loop, not Promise.all)
  </verify>
  <done>
- OllamaTool type matches Ollama function format
- convertMCPToolToOllama converts without strict mode
- Sequential execution for reliability with local LLMs
- validateToolCall catches hallucinated tool calls
- LM Studio format supported via OpenAI conversion
  </done>
</task>

<task type="auto">
  <name>Task 2: Create local LLM HTTP endpoints</name>
  <files>
    apps/omnii_mcp/src/routes/local-llm.ts
    apps/omnii_mcp/src/mcp/adapters/index.ts
    apps/omnii_mcp/src/app.ts
  </files>
  <action>
Create `apps/omnii_mcp/src/routes/local-llm.ts`:

1. **Route definition**:
   ```typescript
   import { Elysia, t } from 'elysia';
   import { getAllToolsForOllama, handleLocalLLMToolCalls, validateToolCall } from '../mcp/adapters/local-llm';
   import { getAllToolsForOpenAI } from '../mcp/adapters/openai';

   export const localLLMRoutes = new Elysia({ prefix: '/api/local-llm' })
   ```

2. **GET /api/local-llm/tools** - Return tools in requested format:
   ```typescript
   .get('/tools', ({ query }) => {
     const format = query.format || 'ollama';

     if (format === 'lmstudio') {
       // LM Studio uses OpenAI format
       return { tools: getAllToolsForOpenAI(), format: 'openai' };
     }

     return { tools: getAllToolsForOllama(), format: 'ollama' };
   }, {
     query: t.Object({
       format: t.Optional(t.Union([t.Literal('ollama'), t.Literal('lmstudio')]))
     })
   })
   ```

3. **POST /api/local-llm/execute-tools** - Execute tool calls:
   ```typescript
   .post('/execute-tools', async ({ body, store }) => {
     const { tool_calls } = body;
     const { userId, neo4jClient } = store;

     // Validate tool calls first (catch hallucinations)
     const validCalls = tool_calls.filter(validateToolCall);
     const invalidCalls = tool_calls.filter(tc => !validateToolCall(tc));

     const results = await handleLocalLLMToolCalls(validCalls, {
       userId,
       client: neo4jClient
     });

     return {
       results,
       invalidCalls: invalidCalls.length > 0 ? {
         count: invalidCalls.length,
         message: 'Some tool calls were invalid (unknown tool name or malformed arguments)'
       } : undefined
     };
   }, {
     body: t.Object({
       tool_calls: t.Array(t.Object({
         function: t.Object({
           name: t.String(),
           arguments: t.Record(t.String(), t.Unknown())
         })
       }))
     })
   })
   ```

4. **GET /api/local-llm/health** - Check local LLM bridge status:
   ```typescript
   .get('/health', () => ({
     status: 'ok',
     supportedFormats: ['ollama', 'lmstudio'],
     toolCount: getAllToolsForOllama().length,
     note: 'For best results, use tool-calling-capable models: Llama 3.1+, Codestral, Qwen, Command R'
   }))
   ```

5. **Update adapters/index.ts**:
   ```typescript
   export * from './openai';
   export * from './local-llm';
   ```

6. **Update app.ts**:
   - Import localLLMRoutes
   - Add to app: `app.use(localLLMRoutes)`
   - Apply auth middleware to execute-tools endpoint
  </action>
  <verify>
Run TypeScript compilation: `cd apps/omnii_mcp && bunx tsc --noEmit`

Check route registration: `grep "localLLMRoutes" apps/omnii_mcp/src/app.ts`

**Test endpoints with actual tool execution:**

1. Start server: `cd apps/omnii_mcp && bun run dev`

2. Test health endpoint:
   ```bash
   curl http://localhost:3000/api/local-llm/health
   # Expected: {"status":"ok","supportedFormats":["ollama","lmstudio"],"toolCount":7,...}
   ```

3. Test Ollama format tools:
   ```bash
   curl http://localhost:3000/api/local-llm/tools | jq '.tools[0].function'
   # Verify: has name, description, parameters (NO strict field)
   ```

4. Test LM Studio format tools:
   ```bash
   curl "http://localhost:3000/api/local-llm/tools?format=lmstudio" | jq '.tools[0].function'
   # Verify: has strict: true (OpenAI format)
   ```

5. Test actual tool call execution (requires auth token):
   ```bash
   curl -X POST http://localhost:3000/api/local-llm/execute-tools \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $TEST_TOKEN" \
     -d '{
       "tool_calls": [{
         "function": {
           "name": "omnii_graph_search_nodes",
           "arguments": {"query": "test", "limit": 5}
         }
       }]
     }'
   # Expected: {"results":[{"role":"tool","content":"..."}]}
   ```

6. Test hallucination detection:
   ```bash
   curl -X POST http://localhost:3000/api/local-llm/execute-tools \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $TEST_TOKEN" \
     -d '{
       "tool_calls": [{
         "function": {
           "name": "hallucinated_tool",
           "arguments": {}
         }
       }]
     }'
   # Expected: {"results":[],"invalidCalls":{"count":1,"message":"..."}}
   ```

7. Verify Ollama vs LM Studio format differences:
   - Ollama: arguments is parsed object, no strict field
   - LM Studio: uses OpenAI format with strict: true
  </verify>
  <done>
- GET /api/local-llm/tools returns tools in ollama or lmstudio format
- POST /api/local-llm/execute-tools handles tool calls with hallucination detection
- Invalid tool calls reported separately in response
- Health endpoint lists supported formats and recommended models
- Routes registered in app.ts with auth middleware
- Tool execution verified with mock data
- Format differences between Ollama and LM Studio validated
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors
2. Ollama tool format distinct from OpenAI (no strict mode)
3. LM Studio uses OpenAI format (reuses existing conversion)
4. Tool calls processed sequentially for local LLM reliability
5. Invalid/hallucinated tool calls detected and reported
6. Both endpoints accessible and functional
</verification>

<success_criteria>
- [ ] convertMCPToolToOllama produces Ollama-compatible tool schema
- [ ] handleLocalLLMToolCalls processes sequentially (not parallel)
- [ ] validateToolCall catches unknown tool names and malformed arguments
- [ ] GET /api/local-llm/tools supports format query parameter
- [ ] POST /api/local-llm/execute-tools reports invalid tool calls separately
- [ ] Health endpoint recommends tool-calling-capable models
</success_criteria>

<output>
After completion, create `.planning/phases/03-graphrag-advanced-mcp/03-06-SUMMARY.md`
</output>
