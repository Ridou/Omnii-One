---
phase: 09-notes-capture-system
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - apps/omnii_mcp/src/notes/parsers/wikilink-parser.ts
  - apps/omnii_mcp/src/notes/parsers/frontmatter-parser.ts
  - apps/omnii_mcp/src/notes/parsers/index.ts
autonomous: true

must_haves:
  truths:
    - "Wikilinks can be extracted from markdown text"
    - "Piped wikilinks [[target|display]] parse correctly"
    - "Frontmatter can be extracted and validated"
  artifacts:
    - path: "apps/omnii_mcp/src/notes/parsers/wikilink-parser.ts"
      provides: "extractWikilinks function returning WikilinkMatch[]"
      exports: ["extractWikilinks", "normalizeTitle", "renderMarkdownWithWikilinks"]
    - path: "apps/omnii_mcp/src/notes/parsers/frontmatter-parser.ts"
      provides: "parseFrontmatter function extracting YAML"
      exports: ["parseFrontmatter", "stringifyFrontmatter"]
    - path: "apps/omnii_mcp/src/notes/parsers/index.ts"
      provides: "Module barrel exports"
  key_links:
    - from: "apps/omnii_mcp/src/notes/parsers/wikilink-parser.ts"
      to: "apps/omnii_mcp/src/notes/types.ts"
      via: "imports WikilinkMatch type"
      pattern: "import.*WikilinkMatch.*from.*types"
---

<objective>
Create wikilink and frontmatter parsers for extracting [[links]] and YAML metadata from notes

Purpose: Enables bidirectional linking by parsing wiki-style links and extracting template metadata from note content
Output: Wikilink parser extracting [[links]] with normalization, frontmatter parser for YAML metadata, markdown renderer with wikilink support
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-notes-capture-system/09-RESEARCH.md
@apps/omnii_mcp/src/notes/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create wikilink parser</name>
  <files>apps/omnii_mcp/src/notes/parsers/wikilink-parser.ts</files>
  <action>
Create wikilink parser that extracts [[links]] from markdown text.

Create `apps/omnii_mcp/src/notes/parsers/wikilink-parser.ts`:

```typescript
/**
 * Wikilink Parser
 *
 * Extracts [[wikilinks]] from markdown content and provides
 * title normalization for consistent database lookups.
 *
 * Handles:
 * - Basic links: [[Note Title]]
 * - Piped links: [[target|display text]]
 * - Nested brackets (escapes them)
 */

import MarkdownIt from 'markdown-it';
import wikilinks from 'markdown-it-wikilinks';
import type { WikilinkMatch } from '../types';

/**
 * Normalize a title for wikilink matching.
 * Converts to lowercase and replaces spaces with hyphens.
 *
 * @param title - Original title string
 * @returns Normalized title for database lookup
 *
 * @example
 * normalizeTitle("Project Alpha") // "project-alpha"
 * normalizeTitle("My Daily Notes") // "my-daily-notes"
 */
export function normalizeTitle(title: string): string {
  return title
    .toLowerCase()
    .trim()
    .replace(/\s+/g, '-')
    .replace(/[^\w-]/g, ''); // Remove special chars except hyphens
}

/**
 * Extract all wikilinks from markdown content.
 *
 * Parses [[wikilinks]] including piped links [[target|display]].
 * Returns deduplicated list of normalized targets.
 *
 * @param markdown - Markdown content to parse
 * @returns Array of WikilinkMatch objects
 *
 * @example
 * extractWikilinks("Check [[Project Alpha]] and [[john-smith|John]]")
 * // Returns:
 * // [
 * //   { raw: "[[Project Alpha]]", target: "Project Alpha", display: "Project Alpha", normalizedTarget: "project-alpha", position: 6 },
 * //   { raw: "[[john-smith|John]]", target: "john-smith", display: "John", normalizedTarget: "john-smith", position: 35 }
 * // ]
 */
export function extractWikilinks(markdown: string): WikilinkMatch[] {
  // Regex to match [[...]] including piped links
  const wikilinkRegex = /\[\[([^\]]+)\]\]/g;
  const matches: WikilinkMatch[] = [];
  const seen = new Set<string>();

  let match;
  while ((match = wikilinkRegex.exec(markdown)) !== null) {
    const raw = match[0];
    const innerText = match[1];
    const position = match.index;

    // Handle piped links: [[target|display]]
    const pipeIndex = innerText.indexOf('|');
    let target: string;
    let display: string;

    if (pipeIndex !== -1) {
      target = innerText.substring(0, pipeIndex).trim();
      display = innerText.substring(pipeIndex + 1).trim();
    } else {
      target = innerText.trim();
      display = target;
    }

    const normalizedTarget = normalizeTitle(target);

    // Deduplicate by normalized target
    if (!seen.has(normalizedTarget)) {
      seen.add(normalizedTarget);
      matches.push({
        raw,
        target,
        display,
        normalizedTarget,
        position,
      });
    }
  }

  return matches;
}

/**
 * Get unique normalized wikilink targets from content.
 * Convenience function when you only need target strings.
 *
 * @param markdown - Markdown content
 * @returns Array of normalized target strings
 */
export function getWikilinkTargets(markdown: string): string[] {
  return extractWikilinks(markdown).map((m) => m.normalizedTarget);
}

/**
 * Configure markdown-it with wikilink rendering support.
 * Returns a renderer that converts [[links]] to HTML anchors.
 */
export function createWikilinkRenderer(): MarkdownIt {
  return new MarkdownIt().use(wikilinks, {
    baseURL: '/notes/',
    relativeBaseURL: './',
    makeAllLinksAbsolute: false,
    uriSuffix: '',
    htmlAttributes: { class: 'wikilink' },
    postProcessPagePath: (pagePath: string) => {
      // Normalize for URL: lowercase, hyphens
      return normalizeTitle(pagePath);
    },
    postProcessLabel: (label: string) => {
      // Display text: preserve original formatting
      return label;
    },
  });
}

/**
 * Render markdown to HTML with wikilink support.
 *
 * @param markdown - Markdown content with [[wikilinks]]
 * @returns HTML string with wikilinks rendered as anchor tags
 */
export function renderMarkdownWithWikilinks(markdown: string): string {
  const md = createWikilinkRenderer();
  return md.render(markdown);
}

/**
 * Replace wikilinks with plain text (for search indexing).
 * Converts [[target|display]] to just "display".
 *
 * @param markdown - Markdown with wikilinks
 * @returns Plain text without wiki brackets
 */
export function stripWikilinks(markdown: string): string {
  return markdown.replace(/\[\[([^\]|]+)(?:\|([^\]]+))?\]\]/g, (_, target, display) => {
    return display || target;
  });
}

/**
 * Count wikilinks in content without parsing full details.
 *
 * @param markdown - Markdown content
 * @returns Number of wikilinks found
 */
export function countWikilinks(markdown: string): number {
  const regex = /\[\[[^\]]+\]\]/g;
  const matches = markdown.match(regex);
  return matches ? matches.length : 0;
}
```
  </action>
  <verify>Run `bun typecheck --filter omnii_mcp` to verify no errors. Test extraction logic manually: `extractWikilinks("[[Test]] and [[other|Other]]")` should return 2 matches.</verify>
  <done>wikilink-parser.ts exports extractWikilinks, normalizeTitle, renderMarkdownWithWikilinks, stripWikilinks, countWikilinks functions</done>
</task>

<task type="auto">
  <name>Task 2: Create frontmatter parser</name>
  <files>apps/omnii_mcp/src/notes/parsers/frontmatter-parser.ts</files>
  <action>
Create frontmatter parser using gray-matter for YAML extraction.

Create `apps/omnii_mcp/src/notes/parsers/frontmatter-parser.ts`:

```typescript
/**
 * Frontmatter Parser
 *
 * Extracts and parses YAML frontmatter from note content.
 * Uses gray-matter for robust parsing of edge cases.
 *
 * Frontmatter format:
 * ---
 * key: value
 * nested:
 *   child: value
 * ---
 * Content starts here...
 */

import matter from 'gray-matter';

/**
 * Result from parsing frontmatter
 */
export interface FrontmatterResult {
  /** Parsed frontmatter as object */
  data: Record<string, unknown>;
  /** Content without frontmatter */
  content: string;
  /** Whether frontmatter was present */
  hasFrontmatter: boolean;
  /** Original frontmatter string (for round-tripping) */
  originalFrontmatter?: string;
}

/**
 * Parse frontmatter from note content.
 *
 * Extracts YAML frontmatter (between --- delimiters) and returns
 * both the parsed data and remaining content.
 *
 * @param noteContent - Full note content with optional frontmatter
 * @returns Parsed frontmatter and content
 *
 * @example
 * parseFrontmatter(`---
 * type: meeting-notes
 * date: 2026-01-29
 * ---
 * # Meeting Notes
 * Content here...`)
 * // Returns:
 * // {
 * //   data: { type: "meeting-notes", date: "2026-01-29" },
 * //   content: "# Meeting Notes\nContent here...",
 * //   hasFrontmatter: true
 * // }
 */
export function parseFrontmatter(noteContent: string): FrontmatterResult {
  try {
    const result = matter(noteContent);

    // Check if there was actual frontmatter
    const hasFrontmatter = Object.keys(result.data).length > 0;

    // Extract original frontmatter for round-tripping
    let originalFrontmatter: string | undefined;
    if (hasFrontmatter) {
      const match = noteContent.match(/^---\n([\s\S]*?)\n---/);
      if (match) {
        originalFrontmatter = match[1];
      }
    }

    return {
      data: result.data,
      content: result.content.trim(),
      hasFrontmatter,
      originalFrontmatter,
    };
  } catch (error) {
    // If parsing fails, return content as-is with empty data
    console.warn('Frontmatter parsing failed:', error);
    return {
      data: {},
      content: noteContent,
      hasFrontmatter: false,
    };
  }
}

/**
 * Create note content with frontmatter.
 *
 * Combines frontmatter data with content into a valid note string.
 *
 * @param data - Frontmatter object
 * @param content - Markdown content
 * @returns Combined string with YAML frontmatter
 */
export function stringifyFrontmatter(
  data: Record<string, unknown>,
  content: string
): string {
  if (Object.keys(data).length === 0) {
    return content;
  }

  return matter.stringify(content, data);
}

/**
 * Update frontmatter while preserving content.
 *
 * @param noteContent - Existing note content
 * @param updates - Fields to add/update in frontmatter
 * @returns Updated note content
 */
export function updateFrontmatter(
  noteContent: string,
  updates: Record<string, unknown>
): string {
  const parsed = parseFrontmatter(noteContent);
  const mergedData = { ...parsed.data, ...updates };
  return stringifyFrontmatter(mergedData, parsed.content);
}

/**
 * Remove frontmatter from content.
 *
 * @param noteContent - Note content with potential frontmatter
 * @returns Content without frontmatter
 */
export function removeFrontmatter(noteContent: string): string {
  const parsed = parseFrontmatter(noteContent);
  return parsed.content;
}

/**
 * Check if content has frontmatter.
 *
 * @param noteContent - Note content to check
 * @returns True if frontmatter present
 */
export function hasFrontmatter(noteContent: string): boolean {
  return noteContent.trimStart().startsWith('---');
}

/**
 * Get specific frontmatter field.
 *
 * @param noteContent - Note content
 * @param field - Field name to extract
 * @returns Field value or undefined
 */
export function getFrontmatterField<T = unknown>(
  noteContent: string,
  field: string
): T | undefined {
  const parsed = parseFrontmatter(noteContent);
  return parsed.data[field] as T | undefined;
}

/**
 * Validate frontmatter has required fields.
 *
 * @param noteContent - Note content
 * @param requiredFields - Array of required field names
 * @returns Object with valid flag and missing fields
 */
export function validateFrontmatter(
  noteContent: string,
  requiredFields: string[]
): { valid: boolean; missingFields: string[] } {
  const parsed = parseFrontmatter(noteContent);
  const missingFields = requiredFields.filter(
    (field) => !(field in parsed.data)
  );

  return {
    valid: missingFields.length === 0,
    missingFields,
  };
}
```
  </action>
  <verify>Run `bun typecheck --filter omnii_mcp`. Test with: `parseFrontmatter("---\ntype: test\n---\nContent")` should return data with type field.</verify>
  <done>frontmatter-parser.ts exports parseFrontmatter, stringifyFrontmatter, updateFrontmatter, removeFrontmatter, validateFrontmatter functions</done>
</task>

<task type="auto">
  <name>Task 3: Create parsers module index</name>
  <files>apps/omnii_mcp/src/notes/parsers/index.ts</files>
  <action>
Create barrel export for parsers module.

Create `apps/omnii_mcp/src/notes/parsers/index.ts`:

```typescript
/**
 * Note Parsers Module
 *
 * Exports wikilink and frontmatter parsing utilities.
 */

// Wikilink parsing
export {
  extractWikilinks,
  getWikilinkTargets,
  normalizeTitle,
  createWikilinkRenderer,
  renderMarkdownWithWikilinks,
  stripWikilinks,
  countWikilinks,
} from './wikilink-parser';

// Frontmatter parsing
export {
  parseFrontmatter,
  stringifyFrontmatter,
  updateFrontmatter,
  removeFrontmatter,
  hasFrontmatter,
  getFrontmatterField,
  validateFrontmatter,
} from './frontmatter-parser';

// Re-export types
export type { FrontmatterResult } from './frontmatter-parser';
```
  </action>
  <verify>Run `bun typecheck --filter omnii_mcp`. Import test: create temp file that imports from `./parsers` and verify all exports resolve.</verify>
  <done>parsers/index.ts barrel exports all parsing functions from wikilink-parser and frontmatter-parser</done>
</task>

</tasks>

<verification>
1. `bun typecheck --filter omnii_mcp` passes
2. `extractWikilinks("[[Test Link]]")` returns array with normalizedTarget "test-link"
3. `parseFrontmatter("---\nkey: value\n---\ncontent")` extracts data object
4. Piped links `[[target|display]]` parse correctly with separate target/display
5. All functions exported from parsers/index.ts
</verification>

<success_criteria>
- Wikilink parser extracts [[links]] with normalization
- Frontmatter parser handles YAML extraction/serialization
- Piped links [[target|display]] parsed correctly
- Module exports clean API via index.ts
- Ready for graph operations in 09-03
</success_criteria>

<output>
After completion, create `.planning/phases/09-notes-capture-system/09-02-SUMMARY.md`
</output>
