---
phase: 09-notes-capture-system
plan: 06
type: execute
wave: 3
depends_on: ["09-01"]
files_modified:
  - apps/omnii-mobile/package.json
  - apps/omnii-mobile/src/features/notes/hooks/useVoiceCapture.ts
  - apps/omnii-mobile/src/features/notes/hooks/index.ts
autonomous: true

must_haves:
  truths:
    - "Voice recording starts on button press"
    - "Interim transcription shows in under 3 seconds"
    - "Final transcription available when recording stops"
    - "On-device recognition used for low latency"
  artifacts:
    - path: "apps/omnii-mobile/src/features/notes/hooks/useVoiceCapture.ts"
      provides: "React hook for voice recording and transcription"
      exports: ["useVoiceCapture"]
    - path: "apps/omnii-mobile/package.json"
      provides: "expo-speech-recognition dependency"
      contains: "expo-speech-recognition"
  key_links:
    - from: "apps/omnii-mobile/src/features/notes/hooks/useVoiceCapture.ts"
      to: "expo-speech-recognition"
      via: "native speech recognition APIs"
      pattern: "from 'expo-speech-recognition'"
---

<objective>
Install expo-speech-recognition and create voice capture hook for mobile note dictation

Purpose: Enables voice-to-text note capture with under 3-second latency using on-device recognition (NOTE-04 requirement)
Output: useVoiceCapture hook with start/stop recording, interim results, and transcription state
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-notes-capture-system/09-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install expo-speech-recognition</name>
  <files>apps/omnii-mobile/package.json</files>
  <action>
Install expo-speech-recognition in the mobile app workspace.

```bash
cd apps/omnii-mobile && npx expo install expo-speech-recognition
```

This package wraps:
- iOS: SFSpeechRecognizer (on-device recognition available iOS 17+)
- Android: SpeechRecognizer (on-device varies by device)

Note: expo-speech-recognition is @jamsch/expo-speech-recognition on npm. The expo install command handles correct version resolution.

After installation, run `bun install` from monorepo root to update lockfile.
  </action>
  <verify>Verify `grep "expo-speech-recognition" apps/omnii-mobile/package.json` shows the dependency. Run `bun install` from root.</verify>
  <done>expo-speech-recognition added to mobile app dependencies</done>
</task>

<task type="auto">
  <name>Task 2: Create voice capture hook</name>
  <files>
apps/omnii-mobile/src/features/notes/hooks/useVoiceCapture.ts
apps/omnii-mobile/src/features/notes/hooks/index.ts
  </files>
  <action>
Create voice capture hook using expo-speech-recognition.

Create directory structure if needed:
```bash
mkdir -p apps/omnii-mobile/src/features/notes/hooks
```

Create `apps/omnii-mobile/src/features/notes/hooks/useVoiceCapture.ts`:

```typescript
/**
 * Voice Capture Hook
 *
 * Provides voice-to-text transcription for note capture.
 * Uses on-device speech recognition for sub-3-second latency.
 *
 * Features:
 * - Interim results for real-time feedback
 * - On-device recognition (iOS 17+, Android varies)
 * - Automatic punctuation
 * - Permission handling
 */

import { useState, useCallback, useEffect } from 'react';
import {
  useSpeechRecognitionEvent,
  ExpoSpeechRecognitionModule,
  getSpeechRecognitionServices,
} from 'expo-speech-recognition';
import type { VoiceTranscriptionResult } from '../types';

/**
 * Voice capture state
 */
export interface VoiceCaptureState {
  /** Current transcript (interim or final) */
  transcript: string;
  /** Whether currently recording */
  isRecording: boolean;
  /** Whether speech recognition is supported */
  isSupported: boolean;
  /** Whether on-device recognition is available */
  hasOnDeviceRecognition: boolean;
  /** Any error message */
  error: string | null;
  /** Whether permission is granted */
  hasPermission: boolean;
  /** Whether this is a final result */
  isFinal: boolean;
}

/**
 * Voice capture actions
 */
export interface VoiceCaptureActions {
  /** Start recording and transcription */
  startRecording: () => Promise<void>;
  /** Stop recording */
  stopRecording: () => void;
  /** Clear current transcript */
  clearTranscript: () => void;
  /** Request microphone permission */
  requestPermission: () => Promise<boolean>;
}

/**
 * Hook for voice-to-text note capture.
 *
 * @param options - Configuration options
 * @returns Voice capture state and actions
 *
 * @example
 * const { transcript, isRecording, startRecording, stopRecording } = useVoiceCapture();
 *
 * // Start recording
 * await startRecording();
 *
 * // Stop and get final transcript
 * stopRecording();
 * console.log(transcript);
 */
export function useVoiceCapture(options?: {
  language?: string;
  continuous?: boolean;
  onResult?: (result: VoiceTranscriptionResult) => void;
  onError?: (error: string) => void;
}): VoiceCaptureState & VoiceCaptureActions {
  const { language = 'en-US', continuous = false, onResult, onError } = options || {};

  // State
  const [transcript, setTranscript] = useState('');
  const [isRecording, setIsRecording] = useState(false);
  const [isSupported, setIsSupported] = useState(true);
  const [hasOnDeviceRecognition, setHasOnDeviceRecognition] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [hasPermission, setHasPermission] = useState(false);
  const [isFinal, setIsFinal] = useState(false);

  // Check support on mount
  useEffect(() => {
    async function checkSupport() {
      try {
        const services = await getSpeechRecognitionServices();
        setIsSupported(services.length > 0);

        // Check for on-device recognition
        const hasOnDevice = services.some(
          (s) => s.includes('onDevice') || s.includes('offline')
        );
        setHasOnDeviceRecognition(hasOnDevice);
      } catch {
        setIsSupported(false);
      }
    }
    checkSupport();
  }, []);

  // Listen for speech recognition results
  useSpeechRecognitionEvent('result', (event) => {
    if (event.results && event.results.length > 0) {
      const result = event.results[0];
      const transcriptText = result.transcript || '';

      setTranscript(transcriptText);
      setIsFinal(event.isFinal || false);

      // Callback for parent component
      if (onResult) {
        onResult({
          transcript: transcriptText,
          isFinal: event.isFinal || false,
          confidence: result.confidence,
          language,
        });
      }

      // If final result, stop recording
      if (event.isFinal && !continuous) {
        setIsRecording(false);
      }
    }
  });

  // Listen for errors
  useSpeechRecognitionEvent('error', (event) => {
    const errorMessage = event.error || 'Speech recognition error';
    setError(errorMessage);
    setIsRecording(false);

    if (onError) {
      onError(errorMessage);
    }
  });

  // Listen for end event
  useSpeechRecognitionEvent('end', () => {
    setIsRecording(false);
  });

  /**
   * Request microphone permission
   */
  const requestPermission = useCallback(async (): Promise<boolean> => {
    try {
      const result = await ExpoSpeechRecognitionModule.requestPermissionsAsync();
      const granted = result.status === 'granted';
      setHasPermission(granted);

      if (!granted) {
        setError('Microphone permission denied');
      }

      return granted;
    } catch (err) {
      setError('Failed to request permission');
      return false;
    }
  }, []);

  /**
   * Start voice recording
   */
  const startRecording = useCallback(async (): Promise<void> => {
    // Clear previous state
    setError(null);
    setIsFinal(false);

    // Check permission
    if (!hasPermission) {
      const granted = await requestPermission();
      if (!granted) {
        return;
      }
    }

    try {
      // Start recognition with optimal settings
      ExpoSpeechRecognitionModule.start({
        lang: language,
        interimResults: true, // Show results as user speaks
        maxAlternatives: 1,
        continuous,

        // iOS-specific: prefer on-device for low latency
        requiresOnDeviceRecognition: false, // Fall back if on-device not available
        addsPunctuation: true, // Automatic punctuation

        // Android-specific
        androidIntentOptions: {
          EXTRA_LANGUAGE_MODEL: 'free_form',
          EXTRA_PARTIAL_RESULTS: true,
        },
      });

      setIsRecording(true);
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Failed to start recording';
      setError(message);

      if (onError) {
        onError(message);
      }
    }
  }, [language, continuous, hasPermission, requestPermission, onError]);

  /**
   * Stop voice recording
   */
  const stopRecording = useCallback((): void => {
    try {
      ExpoSpeechRecognitionModule.stop();
      setIsRecording(false);
    } catch (err) {
      // Ignore stop errors (may already be stopped)
    }
  }, []);

  /**
   * Clear transcript
   */
  const clearTranscript = useCallback((): void => {
    setTranscript('');
    setIsFinal(false);
    setError(null);
  }, []);

  return {
    // State
    transcript,
    isRecording,
    isSupported,
    hasOnDeviceRecognition,
    error,
    hasPermission,
    isFinal,

    // Actions
    startRecording,
    stopRecording,
    clearTranscript,
    requestPermission,
  };
}

/**
 * Helper to check if voice capture is available
 */
export async function isVoiceCaptureAvailable(): Promise<boolean> {
  try {
    const services = await getSpeechRecognitionServices();
    return services.length > 0;
  } catch {
    return false;
  }
}
```

Create `apps/omnii-mobile/src/features/notes/hooks/index.ts`:

```typescript
/**
 * Notes Feature Hooks
 */

export { useVoiceCapture, isVoiceCaptureAvailable } from './useVoiceCapture';
export type { VoiceCaptureState, VoiceCaptureActions } from './useVoiceCapture';
```

Create types file if needed `apps/omnii-mobile/src/features/notes/types.ts`:

```typescript
/**
 * Notes Feature Types
 */

/**
 * Voice transcription result (matches backend type)
 */
export interface VoiceTranscriptionResult {
  /** Transcribed text */
  transcript: string;
  /** Whether this is a final result */
  isFinal: boolean;
  /** Confidence score (0-1) */
  confidence?: number;
  /** Detected language */
  language?: string;
}
```
  </action>
  <verify>Run `bun typecheck --filter omnii-mobile` (if TypeScript configured) or verify files exist with `ls apps/omnii-mobile/src/features/notes/hooks/`.</verify>
  <done>useVoiceCapture hook created with startRecording, stopRecording, interim results, and on-device recognition support</done>
</task>

</tasks>

<verification>
1. expo-speech-recognition in package.json
2. useVoiceCapture hook exports properly
3. Hook handles permission requests
4. Interim results enabled for real-time feedback
5. iOS on-device recognition preferred for latency
6. Error handling for unsupported devices
</verification>

<success_criteria>
- Voice capture works on iOS and Android
- Transcription appears within 3 seconds of speaking
- Interim results show real-time transcription
- Permission handling graceful
- NOTE-04 (voice-to-text <3s latency) requirement met
</success_criteria>

<output>
After completion, create `.planning/phases/09-notes-capture-system/09-06-SUMMARY.md`
</output>
