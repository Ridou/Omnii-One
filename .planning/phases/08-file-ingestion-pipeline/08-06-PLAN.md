---
phase: 08-file-ingestion-pipeline
plan: 06
type: execute
wave: 5
depends_on: ["08-04", "08-05"]
files_modified:
  - apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts
  - apps/omnii_mcp/src/ingestion/jobs/workers.ts
  - apps/omnii_mcp/src/ingestion/sources/files/index.ts
autonomous: true

must_haves:
  truths:
    - "BullMQ worker processes file jobs from queue"
    - "Worker downloads file from Supabase Storage"
    - "Worker parses file using correct parser"
    - "Worker chunks text and scores quality"
    - "Worker creates Document and Chunk nodes in Neo4j"
    - "Processing completes within 30 seconds for typical files"
  artifacts:
    - path: "apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts"
      provides: "File processing worker implementation"
      exports: ["processFileJob"]
    - path: "apps/omnii_mcp/src/ingestion/jobs/workers.ts"
      provides: "Extended workers with file processing"
      contains: "file-processing"
    - path: "apps/omnii_mcp/src/ingestion/sources/files/index.ts"
      provides: "File ingestion module exports"
      exports: ["processFileJob"]
  key_links:
    - from: "apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts"
      to: "apps/omnii_mcp/src/ingestion/sources/files/parsers/index.ts"
      via: "imports parseFile"
      pattern: "import.*parseFile.*from.*parsers"
    - from: "apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts"
      to: "apps/omnii_mcp/src/ingestion/sources/files/chunking/index.ts"
      via: "imports chunkDocument"
      pattern: "import.*chunkDocument.*from.*chunking"
    - from: "apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts"
      to: "apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts"
      via: "imports createDocumentWithChunks"
      pattern: "import.*createDocumentWithChunks.*from.*graph-operations"
---

<objective>
Implement BullMQ worker for file processing.

Purpose: Process queued file jobs by parsing, chunking, scoring, and storing in Neo4j graph.
Output: File worker that orchestrates the full ingestion pipeline.
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-file-ingestion-pipeline/08-RESEARCH.md
@.planning/phases/08-file-ingestion-pipeline/08-01-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-02-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-03-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-04-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-05-SUMMARY.md

@apps/omnii_mcp/src/ingestion/jobs/workers.ts
@apps/omnii_mcp/src/ingestion/jobs/queue.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create file processing worker</name>
  <files>apps/omnii_mcp/src/ingestion/sources/files/file-worker.ts</files>
  <action>
Create file-worker.ts with the file processing logic:

```typescript
/**
 * File Processing Worker
 *
 * Processes queued file jobs through the full ingestion pipeline:
 * 1. Download file from Supabase Storage
 * 2. Parse file to extract text
 * 3. Score extraction quality
 * 4. Chunk text for RAG
 * 5. Create Document and Chunk nodes in Neo4j
 *
 * Each step updates job progress for status tracking.
 */

import { Job } from 'bullmq';
import { createClient } from '@supabase/supabase-js';
import { createClientForUser } from '../../../services/neo4j/http-client';
import { parseFile } from './parsers';
import { chunkDocument } from './chunking';
import { scoreExtraction } from './quality-scorer';
import { createDocumentWithChunks, checkDuplicateDocument } from './graph-operations';
import type { FileProcessingJobData, FileProcessingResult } from './types';
import { env } from '../../../config/env';

// Initialize Supabase client
const supabase = createClient(
  env.OMNII_SUPABASE_URL,
  env.OMNII_SUPABASE_SERVICE_KEY
);

const STORAGE_BUCKET = 'documents';

/**
 * Process a file job from the queue.
 *
 * @param job - BullMQ job with file processing data
 * @returns Processing result with document ID and quality metrics
 */
export async function processFileJob(
  job: Job<FileProcessingJobData>
): Promise<FileProcessingResult> {
  const {
    userId,
    storagePath,
    fileType,
    originalName,
    fileHash,
    fileSize,
    mimeType,
  } = job.data;

  console.log(`[FileWorker] Processing file: ${originalName} (${fileType})`);

  try {
    // Progress: 0% - Starting
    await job.updateProgress(0);

    // Step 1: Get Neo4j client for user
    const client = await createClientForUser(userId);
    if (!client) {
      throw new Error('User database not provisioned');
    }

    // Step 2: Check for duplicate (in case of retry)
    const existingDocId = await checkDuplicateDocument(client, fileHash);
    if (existingDocId) {
      console.log(`[FileWorker] Duplicate detected, document already exists: ${existingDocId}`);
      return {
        success: true,
        documentId: existingDocId,
        chunksCreated: 0,
        error: 'Duplicate - document already processed',
      };
    }

    // Progress: 10% - Downloading file
    await job.updateProgress(10);

    // Step 3: Download file from Supabase Storage
    const { data: fileData, error: downloadError } = await supabase.storage
      .from(STORAGE_BUCKET)
      .download(storagePath);

    if (downloadError || !fileData) {
      throw new Error(`Failed to download file: ${downloadError?.message || 'File not found'}`);
    }

    const buffer = await fileData.arrayBuffer();

    // Progress: 20% - Parsing file
    await job.updateProgress(20);

    // Step 4: Parse file to extract text
    console.log(`[FileWorker] Parsing ${fileType} file...`);
    const parsed = await parseFile(buffer, fileType);

    if (!parsed.text || parsed.text.trim().length === 0) {
      throw new Error('Parsing produced no text content');
    }

    console.log(`[FileWorker] Extracted ${parsed.text.length} characters, parser confidence: ${parsed.confidence}`);

    // Progress: 40% - Scoring quality
    await job.updateProgress(40);

    // Step 5: Score extraction quality
    const quality = scoreExtraction(parsed, fileSize, fileType);
    console.log(`[FileWorker] Quality score: ${(quality.confidence * 100).toFixed(0)}%, needsReview: ${quality.needsReview}`);

    if (quality.warnings.length > 0) {
      console.log(`[FileWorker] Warnings: ${quality.warnings.join(', ')}`);
    }

    // Progress: 50% - Chunking text
    await job.updateProgress(50);

    // Step 6: Chunk text for RAG
    console.log(`[FileWorker] Chunking text...`);
    const chunks = await chunkDocument(parsed.text, fileType);
    console.log(`[FileWorker] Created ${chunks.length} chunks`);

    if (chunks.length === 0) {
      throw new Error('Chunking produced no chunks (text may be too short)');
    }

    // Progress: 60% - Creating graph nodes
    await job.updateProgress(60);

    // Step 7: Create Document and Chunk nodes in Neo4j
    console.log(`[FileWorker] Creating graph nodes...`);
    const result = await createDocumentWithChunks(client, {
      originalName,
      fileType,
      mimeType,
      fileHash,
      storagePath,
      size: fileSize,
      extractedText: parsed.text,
      chunks,
      quality,
    });

    // Progress: 100% - Complete
    await job.updateProgress(100);

    console.log(`[FileWorker] Complete! Document: ${result.documentId}, Chunks: ${result.chunksCreated}`);

    return {
      success: true,
      documentId: result.documentId,
      chunksCreated: result.chunksCreated,
      quality,
    };
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    console.error(`[FileWorker] Failed to process ${originalName}:`, message);

    return {
      success: false,
      error: message,
    };
  }
}
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/sources/files/file-worker.ts --outdir /tmp --target node
```
  </verify>
  <done>
file-worker.ts exports processFileJob with full pipeline orchestration.
File compiles successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate file worker into existing workers module</name>
  <files>apps/omnii_mcp/src/ingestion/jobs/workers.ts</files>
  <action>
Extend the existing workers.ts to include file processing.

1. Add import at top:
```typescript
import { Worker, Job } from 'bullmq';
import type { FileProcessingJobData, FileProcessingResult } from '../sources/files/types';
```

2. Add file worker instance variable near existing syncWorker:
```typescript
let fileWorker: Worker<FileProcessingJobData, FileProcessingResult> | null = null;
```

3. Create startFileWorker function:
```typescript
/**
 * Start the file processing worker
 *
 * @param concurrency - Number of concurrent file processing jobs (default: 2)
 */
export async function startFileWorker(concurrency: number = 2): Promise<void> {
  if (fileWorker) {
    console.warn('File worker already running');
    return;
  }

  const { processFileJob } = await import('../sources/files/file-worker');
  const connection = getRedisConnection();

  fileWorker = new Worker<FileProcessingJobData, FileProcessingResult>(
    'file-processing',
    processFileJob,
    {
      connection,
      concurrency,
      limiter: {
        max: 5, // Max 5 jobs per minute (prevent resource exhaustion)
        duration: 60000,
      },
    }
  );

  // Event handlers
  fileWorker.on('completed', (job, result) => {
    console.log(`[FileWorker] Job ${job.id} completed:`,
      result.success
        ? `Document ${result.documentId}, ${result.chunksCreated} chunks`
        : `Failed: ${result.error}`
    );
  });

  fileWorker.on('failed', (job, error) => {
    console.error(`[FileWorker] Job ${job?.id} failed:`, error.message);
  });

  fileWorker.on('error', (error) => {
    console.error('[FileWorker] Worker error:', error);
  });

  fileWorker.on('progress', (job, progress) => {
    console.log(`[FileWorker] Job ${job.id} progress: ${progress}%`);
  });

  console.log(`File processing worker started with concurrency ${concurrency}`);
}
```

4. Create stopFileWorker function:
```typescript
/**
 * Stop the file processing worker gracefully
 */
export async function stopFileWorker(): Promise<void> {
  if (fileWorker) {
    await fileWorker.close();
    fileWorker = null;
    console.log('File processing worker stopped');
  }
}
```

5. Update startIngestionWorkers to also start file worker:
```typescript
export async function startIngestionWorkers(concurrency: number = 3): Promise<void> {
  // Existing sync worker startup...

  // Start file worker with lower concurrency (file processing is heavier)
  await startFileWorker(2);
}
```

6. Update stopIngestionWorkers to also stop file worker:
```typescript
export async function stopIngestionWorkers(): Promise<void> {
  if (syncWorker) {
    await syncWorker.close();
    syncWorker = null;
    console.log('Ingestion worker stopped');
  }

  await stopFileWorker();
}
```

7. Update getWorkerStatus to include file worker:
```typescript
export function getWorkerStatus(): {
  syncWorker: { running: boolean; concurrency?: number };
  fileWorker: { running: boolean; concurrency?: number };
} {
  return {
    syncWorker: syncWorker
      ? { running: true, concurrency: syncWorker.opts.concurrency }
      : { running: false },
    fileWorker: fileWorker
      ? { running: true, concurrency: fileWorker.opts.concurrency }
      : { running: false },
  };
}
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/jobs/workers.ts --outdir /tmp --target node
```
  </verify>
  <done>
workers.ts includes file worker startup/shutdown integrated with existing ingestion workers.
File compiles successfully.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create files module index</name>
  <files>apps/omnii_mcp/src/ingestion/sources/files/index.ts</files>
  <action>
Create index.ts to export all file ingestion components:

```typescript
/**
 * File Ingestion Module
 *
 * Exports for file upload, parsing, chunking, and processing.
 */

// Types
export * from './types';

// Validators
export { validateFile, calculateFileHash, MAX_FILE_SIZE } from './validators/file-validator';

// Parsers
export { parseFile, parsePDF, parseDOCX, parseText, parseMarkdown, parseCode } from './parsers';

// Chunking
export { chunkDocument, chunkWithConfig, estimateChunkCount } from './chunking';

// Quality scoring
export { scoreExtraction, formatQualityAssessment, isQualityAcceptable, REVIEW_THRESHOLD } from './quality-scorer';

// Graph operations
export {
  createDocumentWithChunks,
  checkDuplicateDocument,
  getDocumentById,
  getDocumentsNeedingReview,
  updateDocumentReviewStatus,
} from './graph-operations';

// Worker
export { processFileJob } from './file-worker';
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/sources/files/index.ts --outdir /tmp --target node
```
  </verify>
  <done>
index.ts exports all file ingestion components.
File compiles successfully.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. processFileJob() orchestrates full pipeline (download -> parse -> score -> chunk -> graph)
2. File worker started alongside sync workers
3. Worker respects concurrency limits (2 concurrent file jobs)
4. Worker respects rate limits (5 jobs per minute)
5. Progress updates visible during processing
6. Module index exports all components
7. All files compile without TypeScript errors
</verification>

<success_criteria>
- processFileJob() downloads from Supabase, parses, chunks, scores, creates nodes
- File worker integrated into existing worker lifecycle
- Progress updates at each pipeline stage
- Error handling with meaningful messages
- Module index exports all components for external use
- Processing completes within 30 seconds for typical files (requirement from FILE-01)
</success_criteria>

<output>
After completion, create `.planning/phases/08-file-ingestion-pipeline/08-06-SUMMARY.md`
</output>
