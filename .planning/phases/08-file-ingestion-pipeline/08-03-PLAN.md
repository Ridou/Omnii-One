---
phase: 08-file-ingestion-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["08-02"]
files_modified:
  - apps/omnii_mcp/src/ingestion/sources/files/chunking/semantic-chunker.ts
  - apps/omnii_mcp/src/ingestion/sources/files/quality-scorer.ts
  - apps/omnii_mcp/src/ingestion/sources/files/chunking/index.ts
autonomous: true

must_haves:
  truths:
    - "Document text can be split into semantic chunks with configurable overlap"
    - "Extraction quality is scored using multiple heuristics"
    - "Low confidence extractions (below 0.8) are flagged for review"
  artifacts:
    - path: "apps/omnii_mcp/src/ingestion/sources/files/chunking/semantic-chunker.ts"
      provides: "RecursiveCharacterTextSplitter-based chunking"
      exports: ["chunkDocument"]
    - path: "apps/omnii_mcp/src/ingestion/sources/files/quality-scorer.ts"
      provides: "Extraction quality scoring with needsReview flag"
      exports: ["scoreExtraction", "QualityMetrics"]
    - path: "apps/omnii_mcp/src/ingestion/sources/files/chunking/index.ts"
      provides: "Chunking exports"
      exports: ["chunkDocument"]
  key_links:
    - from: "apps/omnii_mcp/src/ingestion/sources/files/quality-scorer.ts"
      to: "apps/omnii_mcp/src/ingestion/sources/files/types.ts"
      via: "imports QualityMetrics interface"
      pattern: "import.*QualityMetrics.*from.*types"
---

<objective>
Implement semantic text chunking and extraction quality scoring.

Purpose: Enable RAG-optimized document chunking with overlap, and quality validation to flag poor extractions for human review.
Output: Chunking module using LangChain splitters, quality scorer with configurable thresholds.
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-file-ingestion-pipeline/08-RESEARCH.md
@.planning/phases/08-file-ingestion-pipeline/08-01-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-02-SUMMARY.md

@apps/omnii_mcp/src/ingestion/sources/files/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create semantic chunking module</name>
  <files>
apps/omnii_mcp/src/ingestion/sources/files/chunking/semantic-chunker.ts
apps/omnii_mcp/src/ingestion/sources/files/chunking/index.ts
  </files>
  <action>
Create the chunking directory:
```
mkdir -p apps/omnii_mcp/src/ingestion/sources/files/chunking
```

Create semantic-chunker.ts:

```typescript
/**
 * Semantic Chunking Module
 *
 * Splits documents into semantically coherent chunks for RAG retrieval.
 * Uses LangChain's RecursiveCharacterTextSplitter which preserves
 * semantic boundaries (paragraphs -> sentences -> words).
 *
 * Key parameters from research:
 * - 400-512 tokens per chunk (optimal for retrieval)
 * - 10-20% overlap (prevents context loss at boundaries)
 */

import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { CHUNK_CONFIGS, type SupportedFileType, type ChunkConfig } from '../types';

/**
 * Chunk document text using semantic splitting.
 *
 * Uses RecursiveCharacterTextSplitter which tries to split on:
 * 1. Double newlines (paragraph breaks)
 * 2. Single newlines
 * 3. Periods followed by space (sentence boundaries)
 * 4. Spaces (word boundaries)
 * 5. Characters (last resort)
 *
 * This preserves semantic coherence better than fixed-size chunking.
 *
 * @param text - Document text to chunk
 * @param fileType - File type for config selection
 * @returns Array of text chunks
 */
export async function chunkDocument(
  text: string,
  fileType: SupportedFileType
): Promise<string[]> {
  if (!text || text.trim().length === 0) {
    return [];
  }

  const config = CHUNK_CONFIGS[fileType] || CHUNK_CONFIGS.txt;

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: config.chunkSize,
    chunkOverlap: config.chunkOverlap,
    separators: config.separators,
  });

  const chunks = await splitter.splitText(text);

  // Filter out very short chunks (less than 20 chars - likely noise)
  return chunks.filter((chunk) => chunk.trim().length >= 20);
}

/**
 * Chunk document with custom configuration.
 *
 * @param text - Document text to chunk
 * @param config - Custom chunk configuration
 * @returns Array of text chunks
 */
export async function chunkWithConfig(
  text: string,
  config: ChunkConfig
): Promise<string[]> {
  if (!text || text.trim().length === 0) {
    return [];
  }

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: config.chunkSize,
    chunkOverlap: config.chunkOverlap,
    separators: config.separators,
  });

  const chunks = await splitter.splitText(text);

  return chunks.filter((chunk) => chunk.trim().length >= 20);
}

/**
 * Estimate number of chunks for a given text.
 *
 * Useful for progress indication before processing.
 *
 * @param text - Document text
 * @param fileType - File type for config selection
 * @returns Estimated chunk count
 */
export function estimateChunkCount(text: string, fileType: SupportedFileType): number {
  if (!text || text.trim().length === 0) {
    return 0;
  }

  const config = CHUNK_CONFIGS[fileType] || CHUNK_CONFIGS.txt;
  const effectiveSize = config.chunkSize - config.chunkOverlap;

  return Math.ceil(text.length / effectiveSize);
}
```

Create index.ts:

```typescript
/**
 * Chunking Module Exports
 */

export {
  chunkDocument,
  chunkWithConfig,
  estimateChunkCount,
} from './semantic-chunker';
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/sources/files/chunking/index.ts --outdir /tmp --target node
```
  </verify>
  <done>
semantic-chunker.ts exports chunkDocument with LangChain splitter.
index.ts exports chunking functions.
All files compile successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create extraction quality scorer</name>
  <files>apps/omnii_mcp/src/ingestion/sources/files/quality-scorer.ts</files>
  <action>
Create quality-scorer.ts:

```typescript
/**
 * Extraction Quality Scorer
 *
 * Calculates confidence metrics for document extraction.
 * Critical for FILE-04: Flag low-confidence extractions for review.
 *
 * Research finding: File parsing achieves only 50-70% accuracy "out of the box".
 * Quality scoring is mandatory from day one to prevent silent failures.
 *
 * Threshold: Extractions below 0.8 confidence are flagged for human review.
 */

import type { ParseResult, QualityMetrics, SupportedFileType } from './types';

/** Confidence threshold below which extraction needs human review */
export const REVIEW_THRESHOLD = 0.8;

/** Minimum expected text density (chars per KB of file) */
const MIN_TEXT_DENSITY: Record<SupportedFileType, number> = {
  pdf: 50,    // PDFs can have images, lower threshold
  docx: 100,  // Word docs should have decent text
  txt: 500,   // Text files should be very dense
  md: 300,    // Markdown has some markup overhead
  code: 200,  // Code has whitespace, comments
};

/**
 * Score extraction quality using multiple heuristics.
 *
 * Combines parser confidence with additional checks:
 * - Text density (text length vs file size)
 * - Encoding issues (replacement characters)
 * - Parser-specific warnings
 *
 * @param parsed - Parse result from file parser
 * @param fileSize - Original file size in bytes
 * @param fileType - Detected file type
 * @returns QualityMetrics with confidence and needsReview flag
 */
export function scoreExtraction(
  parsed: ParseResult,
  fileSize: number,
  fileType: SupportedFileType
): QualityMetrics {
  const warnings: string[] = [];
  let confidence = parsed.confidence;

  // Factor 1: Text density (text length relative to file size)
  const textDensity = (parsed.text.length * 1024) / fileSize; // chars per KB
  const expectedDensity = MIN_TEXT_DENSITY[fileType];

  if (textDensity < expectedDensity * 0.5) {
    warnings.push(`Low text density: ${textDensity.toFixed(0)} chars/KB (expected: ${expectedDensity}+)`);
    confidence *= 0.7;
  } else if (textDensity < expectedDensity) {
    warnings.push(`Below average text density: ${textDensity.toFixed(0)} chars/KB`);
    confidence *= 0.9;
  }

  // Factor 2: Encoding issues (replacement characters)
  const replacementCount = (parsed.text.match(/\ufffd/g) || []).length;
  if (replacementCount > 0) {
    const ratio = replacementCount / parsed.text.length;
    if (ratio > 0.01) {
      warnings.push(`Encoding issues: ${replacementCount} replacement characters found`);
      confidence *= 0.8;
    } else {
      warnings.push(`Minor encoding issues: ${replacementCount} replacement characters`);
      confidence *= 0.95;
    }
  }

  // Factor 3: Parser warnings (from metadata)
  if (parsed.metadata.warnings && parsed.metadata.warnings.length > 0) {
    warnings.push(...parsed.metadata.warnings);
    // Already factored into parser confidence, but log for visibility
  }

  // Factor 4: Very short extracted text
  if (parsed.text.length < 100) {
    warnings.push(`Very short extraction: only ${parsed.text.length} characters`);
    confidence *= 0.6;
  }

  // Factor 5: Excessive whitespace
  const whitespaceRatio = (parsed.text.match(/\s/g) || []).length / parsed.text.length;
  if (whitespaceRatio > 0.6) {
    warnings.push(`High whitespace ratio: ${(whitespaceRatio * 100).toFixed(0)}%`);
    confidence *= 0.85;
  }

  // Clamp confidence to valid range
  confidence = Math.max(0.1, Math.min(1, confidence));

  // Estimate completeness based on text density
  const completeness = Math.min(1, textDensity / (expectedDensity * 1.5));

  return {
    confidence,
    completeness,
    warnings,
    needsReview: confidence < REVIEW_THRESHOLD,
  };
}

/**
 * Format quality metrics for display.
 *
 * @param metrics - Quality metrics
 * @returns Human-readable quality assessment
 */
export function formatQualityAssessment(metrics: QualityMetrics): string {
  const level = metrics.confidence >= 0.9 ? 'high'
    : metrics.confidence >= 0.8 ? 'medium'
    : metrics.confidence >= 0.6 ? 'low'
    : 'very low';

  let assessment = `Quality: ${level} (${(metrics.confidence * 100).toFixed(0)}% confidence)`;

  if (metrics.needsReview) {
    assessment += ' - NEEDS REVIEW';
  }

  if (metrics.warnings.length > 0) {
    assessment += `\nWarnings:\n- ${metrics.warnings.join('\n- ')}`;
  }

  return assessment;
}

/**
 * Check if quality is acceptable for automatic processing.
 *
 * @param metrics - Quality metrics
 * @returns true if quality is good enough to proceed without review
 */
export function isQualityAcceptable(metrics: QualityMetrics): boolean {
  return !metrics.needsReview;
}
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/sources/files/quality-scorer.ts --outdir /tmp --target node
```
  </verify>
  <done>
quality-scorer.ts exports scoreExtraction with multiple heuristics.
REVIEW_THRESHOLD is 0.8 (80% confidence).
needsReview flag set when confidence < 0.8.
File compiles successfully.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. chunkDocument() splits text using RecursiveCharacterTextSplitter
2. Chunk sizes follow research recommendations (400-512 tokens, 10-20% overlap)
3. scoreExtraction() combines parser confidence with additional heuristics
4. needsReview is true when confidence < 0.8
5. Quality warnings are collected and returned
6. All files compile without TypeScript errors
</verification>

<success_criteria>
- chunkDocument() uses LangChain RecursiveCharacterTextSplitter
- Chunk configs use 512 chars with 100 char overlap (per research)
- scoreExtraction() checks text density, encoding issues, parser warnings
- needsReview flag triggers at <80% confidence
- formatQualityAssessment() provides human-readable output
- All exports work correctly
</success_criteria>

<output>
After completion, create `.planning/phases/08-file-ingestion-pipeline/08-03-SUMMARY.md`
</output>
