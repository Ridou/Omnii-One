---
phase: 08-file-ingestion-pipeline
plan: 07
type: execute
wave: 6
depends_on: ["08-06"]
files_modified:
  - apps/omnii_mcp/src/graph/operations/search.ts
autonomous: false

must_haves:
  truths:
    - "User can search file contents via existing search_nodes MCP tool"
    - "Document and Chunk nodes appear in search results"
    - "Search returns relevant passages from uploaded files"
  artifacts:
    - path: "apps/omnii_mcp/src/graph/operations/search.ts"
      provides: "Extended search to include Document and Chunk nodes"
      contains: "Chunk"
  key_links:
    - from: "apps/omnii_mcp/src/graph/operations/search.ts"
      to: "apps/omnii_mcp/src/graph/schema/nodes.ts"
      via: "includes Document and Chunk in searchable labels"
      pattern: "Document|Chunk"
---

<objective>
Wire file content into existing search infrastructure and verify end-to-end functionality.

Purpose: Enable users to search across all uploaded file contents using the existing search_nodes MCP tool.
Output: Extended search operations, end-to-end verification of file ingestion pipeline.
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-file-ingestion-pipeline/08-RESEARCH.md
@.planning/phases/08-file-ingestion-pipeline/08-01-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-06-SUMMARY.md

@apps/omnii_mcp/src/graph/operations/search.ts
@apps/omnii_mcp/src/mcp/tools/search-nodes.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend search operations to include Document and Chunk nodes</name>
  <files>apps/omnii_mcp/src/graph/operations/search.ts</files>
  <action>
Update search.ts to include Document and Chunk nodes in search results.

1. Update the textBasedSearch function's MATCH clause to include Document and Chunk labels:

Find this line (or similar):
```typescript
WHERE (node:Contact OR node:Event OR node:Entity OR node:Concept)
```

Update to:
```typescript
WHERE (node:Contact OR node:Event OR node:Entity OR node:Concept OR node:Document OR node:Chunk)
```

2. The vector search (searchByEmbedding) should already work since:
   - Chunks have embeddings stored
   - The vector index queries all nodes with embeddings
   - No label filter is applied by default

3. Update the search result properties handling to include chunk-specific fields:

In the searchByText or searchByEmbedding results, ensure the properties include relevant fields for chunks:
- `text` (chunk content)
- `position` (chunk order)
- `documentId` (parent document reference)

The existing code returns `properties(node) AS properties` which should already include these.

4. Consider adding a new search option for file-specific search:

Add to SearchOptions interface (if not exists):
```typescript
/** Include file content (Documents and Chunks) in results */
includeFileContent?: boolean;
```

This is optional - the default behavior should include all node types.
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/graph/operations/search.ts --outdir /tmp --target node
```
  </verify>
  <done>
search.ts includes Document and Chunk nodes in text-based search.
Vector search already works for nodes with embeddings.
File compiles successfully.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end verification of file ingestion pipeline</name>
  <what-built>
Complete file ingestion pipeline:
- File upload endpoint (/api/files/upload)
- File validation with magic number detection
- PDF, Word, text, markdown parsing
- Semantic chunking with overlap
- Quality scoring with review flags
- Document/Chunk graph nodes with embeddings
- Search integration for file content
  </what-built>
  <how-to-verify>
1. Start the development server:
```
cd apps/omnii_mcp && bun run dev
```

2. Ensure Redis is running (for BullMQ):
```
redis-cli ping
# Should return PONG
```

3. Test file upload with a PDF:
```bash
curl -X POST http://localhost:8081/api/files/upload \
  -F "file=@/path/to/test.pdf" \
  -F "userId=test-user-id"
```

Expected response:
```json
{
  "status": "processing",
  "fileHash": "abc123...",
  "message": "File uploaded successfully. Processing in background."
}
```

4. Check processing status:
```bash
curl "http://localhost:8081/api/files/status/{fileHash}?userId=test-user-id"
```

Expected (after ~30 seconds):
```json
{
  "status": "complete",
  "fileHash": "abc123...",
  "documentId": "uuid-here",
  "document": { ... }
}
```

5. Search for content from the uploaded file:
```bash
curl -X POST http://localhost:8081/api/graph/search \
  -H "Content-Type: application/json" \
  -d '{"query": "some text from your PDF", "userId": "test-user-id"}'
```

Expected: Results include chunks from the uploaded document.

6. Check documents needing review (if any had low confidence):
```bash
curl "http://localhost:8081/api/files/needs-review?userId=test-user-id"
```

7. Verify requirements met:
- [ ] FILE-01: PDF uploaded, text extracted, visible in graph within 30 seconds
- [ ] FILE-02: Test with a .docx file (same flow)
- [ ] FILE-03: Test with a .txt or .md file (same flow)
- [ ] FILE-04: Check extractionConfidence and needsReview on document
- [ ] FILE-05: Search returns relevant chunks from uploaded files

Note: For testing, you can use any PDF/DOCX/TXT file you have available.
  </how-to-verify>
  <resume-signal>Type "verified" if all requirements pass, or describe any issues encountered.</resume-signal>
</task>

</tasks>

<verification>
After completing all tasks:
1. Search includes Document and Chunk nodes
2. File upload -> processing -> graph creation works end-to-end
3. Search returns chunks from uploaded files
4. Quality scoring flags low-confidence extractions
5. All FILE-* requirements satisfied
</verification>

<success_criteria>
- FILE-01: User can upload PDF and see extracted text in graph within 30 seconds
- FILE-02: User can upload Word documents with content indexed
- FILE-03: User can upload text/markdown/code files with content indexed
- FILE-04: System displays extraction quality score and flags low-confidence extractions
- FILE-05: User can search file contents via existing search_nodes MCP tool
</success_criteria>

<output>
After completion, create `.planning/phases/08-file-ingestion-pipeline/08-07-SUMMARY.md`
</output>
