---
phase: 08-file-ingestion-pipeline
plan: 05
type: execute
wave: 4
depends_on: ["08-03"]
files_modified:
  - apps/omnii_mcp/src/routes/files.ts
  - apps/omnii_mcp/src/routes/index.ts
autonomous: true

must_haves:
  truths:
    - "User can upload files via multipart POST endpoint"
    - "Files are validated using magic number detection before processing"
    - "Files are stored in Supabase Storage with user-scoped paths"
    - "Processing job is queued immediately after upload"
    - "Duplicate files return existing document reference"
  artifacts:
    - path: "apps/omnii_mcp/src/routes/files.ts"
      provides: "File upload endpoint with validation and queuing"
      exports: ["fileRoutes"]
    - path: "apps/omnii_mcp/src/routes/index.ts"
      provides: "Routes index with file routes registered"
      contains: "fileRoutes"
  key_links:
    - from: "apps/omnii_mcp/src/routes/files.ts"
      to: "apps/omnii_mcp/src/ingestion/sources/files/validators/file-validator.ts"
      via: "imports validateFile, calculateFileHash"
      pattern: "import.*validateFile.*from"
    - from: "apps/omnii_mcp/src/routes/files.ts"
      to: "apps/omnii_mcp/src/ingestion/jobs/queue.ts"
      via: "imports createIngestionQueue for job queuing"
      pattern: "import.*createIngestionQueue"
---

<objective>
Implement file upload endpoint with validation, storage, and job queuing.

Purpose: Enable users to upload files via HTTP API with immediate validation and async processing.
Output: Elysia file upload routes with multipart handling, Supabase Storage integration, BullMQ job queuing.
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-file-ingestion-pipeline/08-RESEARCH.md
@.planning/phases/08-file-ingestion-pipeline/08-01-SUMMARY.md
@.planning/phases/08-file-ingestion-pipeline/08-02-SUMMARY.md

@apps/omnii_mcp/src/routes/ingestion/index.ts
@apps/omnii_mcp/src/ingestion/jobs/queue.ts
@apps/omnii_mcp/src/routes/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create file upload routes</name>
  <files>apps/omnii_mcp/src/routes/files.ts</files>
  <action>
Create files.ts with Elysia file upload endpoint:

```typescript
/**
 * File Upload Routes
 *
 * Handles multipart file uploads with:
 * 1. Magic number validation (security)
 * 2. Supabase Storage upload
 * 3. BullMQ job queuing for async processing
 *
 * Pattern: Upload returns immediately, processing happens in background.
 */

import { Elysia, t } from 'elysia';
import { createClient } from '@supabase/supabase-js';
import { createIngestionQueue } from '../ingestion/jobs/queue';
import { validateFile, calculateFileHash, MAX_FILE_SIZE } from '../ingestion/sources/files/validators/file-validator';
import { checkDuplicateDocument } from '../ingestion/sources/files/graph-operations';
import { createClientForUser } from '../services/neo4j/http-client';
import type { FileProcessingJobData, FileUploadResponse } from '../ingestion/sources/files/types';
import { env } from '../config/env';

// Initialize Supabase client for storage
const supabase = createClient(
  env.OMNII_SUPABASE_URL,
  env.OMNII_SUPABASE_SERVICE_KEY
);

// Storage bucket name
const STORAGE_BUCKET = 'documents';

// Create queue for file processing jobs
const fileQueue = createIngestionQueue('file-processing');

/**
 * File upload routes
 */
export const fileRoutes = new Elysia({ prefix: '/files' })
  /**
   * Upload a file for processing
   *
   * POST /api/files/upload
   * Body: multipart/form-data with 'file' field
   *
   * Returns immediately with processing status.
   * File is processed asynchronously via BullMQ worker.
   */
  .post(
    '/upload',
    async ({ body, set }) => {
      const { file, userId } = body;

      // Read file content
      const buffer = await file.arrayBuffer();
      const fileSize = buffer.byteLength;

      // Step 1: Validate file type using magic numbers (security-critical)
      const validation = await validateFile(buffer, file.name, fileSize);

      if (!validation.valid) {
        set.status = 400;
        return {
          status: 'error',
          fileHash: '',
          message: validation.error || 'File validation failed',
        } satisfies FileUploadResponse;
      }

      // Step 2: Calculate file hash for deduplication
      const fileHash = await calculateFileHash(buffer);

      // Step 3: Check for duplicate (same content already processed)
      try {
        const client = await createClientForUser(userId);
        if (client) {
          const existingDocId = await checkDuplicateDocument(client, fileHash);
          if (existingDocId) {
            return {
              status: 'duplicate',
              fileHash,
              message: 'File already uploaded and processed',
              existingDocumentId: existingDocId,
            } satisfies FileUploadResponse;
          }
        }
      } catch (error) {
        // If Neo4j check fails, continue with upload (dedup is optimization, not requirement)
        console.warn('Deduplication check failed:', error);
      }

      // Step 4: Upload to Supabase Storage
      const storagePath = `${userId}/${fileHash}.${validation.fileType}`;

      const { error: storageError } = await supabase.storage
        .from(STORAGE_BUCKET)
        .upload(storagePath, buffer, {
          contentType: validation.mimeType,
          upsert: false, // Prevent overwrite
        });

      if (storageError) {
        // If file already exists in storage, that's OK (same hash)
        if (!storageError.message?.includes('already exists')) {
          set.status = 500;
          return {
            status: 'error',
            fileHash,
            message: `Storage upload failed: ${storageError.message}`,
          } satisfies FileUploadResponse;
        }
      }

      // Step 5: Queue processing job
      const jobData: FileProcessingJobData = {
        userId,
        storagePath,
        fileType: validation.fileType!,
        originalName: file.name,
        fileHash,
        fileSize,
        mimeType: validation.mimeType!,
      };

      await fileQueue.add('process-file', jobData, {
        attempts: 3,
        backoff: { type: 'exponential', delay: 2000 },
        jobId: `file-${fileHash}`, // Deduplicate jobs by hash
      });

      // Step 6: Return immediately
      return {
        status: 'processing',
        fileHash,
        message: 'File uploaded successfully. Processing in background.',
      } satisfies FileUploadResponse;
    },
    {
      body: t.Object({
        file: t.File({
          maxSize: MAX_FILE_SIZE,
        }),
        userId: t.String({ minLength: 1 }),
      }),
    }
  )

  /**
   * Get file processing status
   *
   * GET /api/files/status/:fileHash
   *
   * Returns processing status and document details if complete.
   */
  .get(
    '/status/:fileHash',
    async ({ params, query, set }) => {
      const { fileHash } = params;
      const { userId } = query;

      if (!userId) {
        set.status = 400;
        return { error: 'userId query parameter required' };
      }

      try {
        // Check if document exists in Neo4j (processing complete)
        const client = await createClientForUser(userId);
        if (!client) {
          set.status = 400;
          return { error: 'User database not provisioned' };
        }

        const existingDocId = await checkDuplicateDocument(client, fileHash);

        if (existingDocId) {
          // Document exists - processing complete
          const { getDocumentById } = await import('../ingestion/sources/files/graph-operations');
          const document = await getDocumentById(client, existingDocId);

          return {
            status: 'complete',
            fileHash,
            documentId: existingDocId,
            document,
          };
        }

        // Check job status in queue
        const job = await fileQueue.getJob(`file-${fileHash}`);

        if (!job) {
          return {
            status: 'not_found',
            fileHash,
            message: 'No file with this hash found in processing queue',
          };
        }

        const state = await job.getState();

        return {
          status: state,
          fileHash,
          progress: job.progress,
          attemptsMade: job.attemptsMade,
        };
      } catch (error) {
        set.status = 500;
        return {
          error: 'Failed to get status',
          details: error instanceof Error ? error.message : String(error),
        };
      }
    },
    {
      params: t.Object({
        fileHash: t.String({ minLength: 64, maxLength: 64 }), // SHA-256 is 64 hex chars
      }),
      query: t.Object({
        userId: t.String({ minLength: 1 }),
      }),
    }
  )

  /**
   * List documents needing review
   *
   * GET /api/files/needs-review
   *
   * Returns documents with low extraction confidence.
   */
  .get(
    '/needs-review',
    async ({ query, set }) => {
      const { userId, limit } = query;

      try {
        const client = await createClientForUser(userId);
        if (!client) {
          set.status = 400;
          return { error: 'User database not provisioned' };
        }

        const { getDocumentsNeedingReview } = await import('../ingestion/sources/files/graph-operations');
        const documents = await getDocumentsNeedingReview(client, limit || 20);

        return {
          count: documents.length,
          documents,
        };
      } catch (error) {
        set.status = 500;
        return {
          error: 'Failed to get documents',
          details: error instanceof Error ? error.message : String(error),
        };
      }
    },
    {
      query: t.Object({
        userId: t.String({ minLength: 1 }),
        limit: t.Optional(t.Number({ minimum: 1, maximum: 100 })),
      }),
    }
  )

  /**
   * Mark document as reviewed
   *
   * PATCH /api/files/:documentId/review
   *
   * Updates document review status after manual verification.
   */
  .patch(
    '/:documentId/review',
    async ({ params, body, set }) => {
      const { documentId } = params;
      const { userId, approved, newConfidence } = body;

      try {
        const client = await createClientForUser(userId);
        if (!client) {
          set.status = 400;
          return { error: 'User database not provisioned' };
        }

        const { updateDocumentReviewStatus } = await import('../ingestion/sources/files/graph-operations');

        await updateDocumentReviewStatus(client, documentId, {
          needsReview: !approved,
          extractionConfidence: newConfidence,
        });

        return {
          success: true,
          message: approved
            ? 'Document marked as reviewed and approved'
            : 'Document marked as still needing review',
        };
      } catch (error) {
        set.status = 500;
        return {
          error: 'Failed to update review status',
          details: error instanceof Error ? error.message : String(error),
        };
      }
    },
    {
      params: t.Object({
        documentId: t.String({ minLength: 1 }),
      }),
      body: t.Object({
        userId: t.String({ minLength: 1 }),
        approved: t.Boolean(),
        newConfidence: t.Optional(t.Number({ minimum: 0, maximum: 1 })),
      }),
    }
  );
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/routes/files.ts --outdir /tmp --target node
```
  </verify>
  <done>
files.ts exports fileRoutes with upload, status, needs-review, and review endpoints.
File compiles successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Register file routes in routes index</name>
  <files>apps/omnii_mcp/src/routes/index.ts</files>
  <action>
Update the routes index to include file routes.

1. Import fileRoutes:
```typescript
import { fileRoutes } from './files';
```

2. Add fileRoutes to the routes array or Elysia app composition.

Look at how other routes (like ingestionRoutes) are registered and follow the same pattern.
  </action>
  <verify>
Check that routes/index.ts imports and exports/uses fileRoutes.
  </verify>
  <done>
routes/index.ts includes fileRoutes in the application.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. POST /api/files/upload accepts multipart file upload
2. File validation uses magic numbers, not extension
3. Files stored in Supabase Storage at `{userId}/{hash}.{ext}`
4. Processing job queued with FileProcessingJobData
5. Duplicate files return existing document reference
6. GET /api/files/status/:fileHash returns processing status
7. GET /api/files/needs-review returns low-confidence documents
8. PATCH /api/files/:documentId/review updates review status
</verification>

<success_criteria>
- fileRoutes registered and accessible at /api/files/*
- Upload validates file using file-type magic numbers
- Upload stores file in Supabase Storage
- Upload queues BullMQ job for processing
- Duplicate detection returns existing document
- Status endpoint shows job progress/completion
- Review endpoints support human-in-the-loop workflow
</success_criteria>

<output>
After completion, create `.planning/phases/08-file-ingestion-pipeline/08-05-SUMMARY.md`
</output>
