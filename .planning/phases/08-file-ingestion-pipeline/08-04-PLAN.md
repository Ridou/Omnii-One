---
phase: 08-file-ingestion-pipeline
plan: 04
type: execute
wave: 4
depends_on: ["08-03"]
files_modified:
  - apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts
  - apps/omnii_mcp/src/graph/schema/constraints.ts
autonomous: true

must_haves:
  truths:
    - "Document nodes can be created in Neo4j with proper properties"
    - "Chunk nodes can be created with embeddings and linked to parent Document"
    - "Sequential chunks are linked with NEXT_CHUNK relationships"
    - "Hash-based deduplication prevents duplicate documents"
  artifacts:
    - path: "apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts"
      provides: "Document and Chunk CRUD operations"
      exports: ["createDocumentWithChunks", "checkDuplicateDocument", "getDocumentById"]
    - path: "apps/omnii_mcp/src/graph/schema/constraints.ts"
      provides: "Document fileHash uniqueness constraint"
      contains: "Document"
  key_links:
    - from: "apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts"
      to: "apps/omnii_mcp/src/graph/operations/embeddings.ts"
      via: "imports generateEmbeddings for chunk vectors"
      pattern: "import.*generateEmbeddings.*from.*embeddings"
    - from: "apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts"
      to: "apps/omnii_mcp/src/services/neo4j/http-client.ts"
      via: "uses Neo4jHTTPClient for queries"
      pattern: "Neo4jHTTPClient"
---

<objective>
Implement Neo4j graph operations for Document and Chunk nodes.

Purpose: Store extracted document content in the graph with embeddings for vector search and relationships for traversal.
Output: Graph operations for document creation, chunk creation with embeddings, deduplication checks.
</objective>

<execution_context>
@/Users/santino/.claude/get-shit-done/workflows/execute-plan.md
@/Users/santino/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-file-ingestion-pipeline/08-RESEARCH.md
@.planning/phases/08-file-ingestion-pipeline/08-01-SUMMARY.md

@apps/omnii_mcp/src/graph/schema/nodes.ts
@apps/omnii_mcp/src/graph/operations/embeddings.ts
@apps/omnii_mcp/src/graph/operations/crud.ts
@apps/omnii_mcp/src/graph/schema/constraints.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Document constraints to schema</name>
  <files>apps/omnii_mcp/src/graph/schema/constraints.ts</files>
  <action>
Extend the existing constraints.ts file to add Document and Chunk constraints.

Add the following constraint definitions to the CONSTRAINTS array:

```typescript
// Document constraints
{
  name: 'document_id_unique',
  cypher: 'CREATE CONSTRAINT document_id_unique IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE',
},
{
  name: 'document_hash_unique',
  cypher: 'CREATE CONSTRAINT document_hash_unique IF NOT EXISTS FOR (d:Document) REQUIRE d.fileHash IS UNIQUE',
},
// Chunk constraints
{
  name: 'chunk_id_unique',
  cypher: 'CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE',
},
```

Update the setupSchemaConstraints function to include these new constraints.
  </action>
  <verify>
Review constraints.ts includes document_id_unique, document_hash_unique, and chunk_id_unique constraints.
  </verify>
  <done>
constraints.ts includes Document and Chunk uniqueness constraints.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create graph operations for Document and Chunk nodes</name>
  <files>apps/omnii_mcp/src/ingestion/sources/files/graph-operations.ts</files>
  <action>
Create graph-operations.ts with Neo4j operations for documents and chunks:

```typescript
/**
 * Graph Operations for File Ingestion
 *
 * Creates Document and Chunk nodes in Neo4j with proper relationships:
 * - (Document)-[:HAS_CHUNK]->(Chunk)
 * - (Chunk)-[:NEXT_CHUNK]->(Chunk)
 *
 * Chunks include embeddings for vector search.
 */

import { v4 as uuid } from 'uuid';
import type { Neo4jHTTPClient } from '../../../services/neo4j/http-client';
import { generateEmbeddings } from '../../../graph/operations/embeddings';
import type { DocumentNode, ChunkNode } from '../../../graph/schema/nodes';
import type { QualityMetrics, SupportedFileType } from './types';

/**
 * Input for creating a document with its chunks.
 */
export interface CreateDocumentInput {
  originalName: string;
  fileType: SupportedFileType;
  mimeType: string;
  fileHash: string;
  storagePath: string;
  size: number;
  extractedText: string;
  chunks: string[];
  quality: QualityMetrics;
}

/**
 * Result from document creation.
 */
export interface CreateDocumentResult {
  documentId: string;
  chunkIds: string[];
  chunksCreated: number;
}

/**
 * Check if a document with the given hash already exists.
 *
 * Used for deduplication before processing.
 *
 * @param client - Neo4j HTTP client
 * @param fileHash - SHA-256 hash of file content
 * @returns Document ID if exists, null otherwise
 */
export async function checkDuplicateDocument(
  client: Neo4jHTTPClient,
  fileHash: string
): Promise<string | null> {
  const cypher = `
    MATCH (d:Document {fileHash: $fileHash})
    RETURN d.id AS id
    LIMIT 1
  `;

  try {
    const result = await client.query(cypher, { fileHash });

    if (result.data?.values?.length > 0) {
      const fields = result.data.fields;
      const idIndex = fields.indexOf('id');
      return result.data.values[0][idIndex] as string;
    }

    return null;
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to check for duplicate document: ${message}`);
  }
}

/**
 * Create a Document node with all its Chunk nodes.
 *
 * Creates:
 * 1. Document node with metadata and quality scores
 * 2. Chunk nodes with text and embeddings
 * 3. HAS_CHUNK relationships (Document -> Chunks)
 * 4. NEXT_CHUNK relationships (Chunk -> Chunk, sequential)
 *
 * @param client - Neo4j HTTP client
 * @param input - Document and chunk data
 * @returns Created document and chunk IDs
 */
export async function createDocumentWithChunks(
  client: Neo4jHTTPClient,
  input: CreateDocumentInput
): Promise<CreateDocumentResult> {
  const documentId = uuid();
  const now = new Date().toISOString();

  // Step 1: Create Document node
  const docCypher = `
    CREATE (d:Document {
      id: $id,
      name: $name,
      originalName: $originalName,
      fileType: $fileType,
      mimeType: $mimeType,
      fileHash: $fileHash,
      storagePath: $storagePath,
      size: $size,
      extractionConfidence: $confidence,
      needsReview: $needsReview,
      uploadedAt: datetime($uploadedAt),
      createdAt: datetime($createdAt),
      chunkCount: $chunkCount
    })
    RETURN d.id AS id
  `;

  try {
    await client.query(docCypher, {
      id: documentId,
      name: input.originalName,
      originalName: input.originalName,
      fileType: input.fileType,
      mimeType: input.mimeType,
      fileHash: input.fileHash,
      storagePath: input.storagePath,
      size: input.size,
      confidence: input.quality.confidence,
      needsReview: input.quality.needsReview,
      uploadedAt: now,
      createdAt: now,
      chunkCount: input.chunks.length,
    });

    // Step 2: Generate embeddings for all chunks
    let embeddings: number[][] = [];
    if (input.chunks.length > 0) {
      embeddings = await generateEmbeddings(input.chunks);
    }

    // Step 3: Create chunk nodes with embeddings and relationships
    const chunkIds: string[] = [];

    if (input.chunks.length > 0) {
      // Prepare chunk data
      const chunksData = input.chunks.map((text, index) => ({
        id: uuid(),
        text,
        position: index,
        embedding: embeddings[index],
        createdAt: now,
      }));

      chunkIds.push(...chunksData.map((c) => c.id));

      // Create all chunks and HAS_CHUNK relationships in a single query
      const chunksCypher = `
        MATCH (d:Document {id: $docId})
        UNWIND $chunks AS chunk
        CREATE (c:Chunk {
          id: chunk.id,
          name: 'Chunk ' + toString(chunk.position + 1),
          text: chunk.text,
          position: chunk.position,
          embedding: chunk.embedding,
          documentId: $docId,
          createdAt: datetime(chunk.createdAt)
        })
        CREATE (d)-[:HAS_CHUNK]->(c)
        RETURN c.id AS id
      `;

      await client.query(chunksCypher, {
        docId: documentId,
        chunks: chunksData,
      });

      // Step 4: Create NEXT_CHUNK relationships between sequential chunks
      if (chunksData.length > 1) {
        const nextChunkCypher = `
          MATCH (d:Document {id: $docId})-[:HAS_CHUNK]->(c:Chunk)
          WITH c ORDER BY c.position
          WITH collect(c) AS chunks
          UNWIND range(0, size(chunks) - 2) AS i
          WITH chunks[i] AS current, chunks[i + 1] AS next
          CREATE (current)-[:NEXT_CHUNK]->(next)
        `;

        await client.query(nextChunkCypher, { docId: documentId });
      }
    }

    return {
      documentId,
      chunkIds,
      chunksCreated: chunkIds.length,
    };
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to create document with chunks: ${message}`);
  }
}

/**
 * Get a document by ID with its metadata.
 *
 * @param client - Neo4j HTTP client
 * @param documentId - Document UUID
 * @returns Document node or null
 */
export async function getDocumentById(
  client: Neo4jHTTPClient,
  documentId: string
): Promise<DocumentNode | null> {
  const cypher = `
    MATCH (d:Document {id: $id})
    RETURN d {
      .id, .name, .originalName, .fileType, .mimeType, .fileHash,
      .storagePath, .size, .extractionConfidence, .needsReview,
      .chunkCount, .createdAt,
      uploadedAt: toString(d.uploadedAt)
    } AS doc
  `;

  try {
    const result = await client.query(cypher, { id: documentId });

    if (!result.data?.values?.length) {
      return null;
    }

    const fields = result.data.fields;
    const docIndex = fields.indexOf('doc');
    return result.data.values[0][docIndex] as DocumentNode;
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to get document: ${message}`);
  }
}

/**
 * Get documents that need review (low confidence extraction).
 *
 * @param client - Neo4j HTTP client
 * @param limit - Maximum documents to return
 * @returns Documents flagged for review
 */
export async function getDocumentsNeedingReview(
  client: Neo4jHTTPClient,
  limit: number = 20
): Promise<DocumentNode[]> {
  const cypher = `
    MATCH (d:Document {needsReview: true})
    RETURN d {
      .id, .name, .originalName, .fileType, .extractionConfidence,
      .needsReview, .chunkCount, .createdAt,
      uploadedAt: toString(d.uploadedAt)
    } AS doc
    ORDER BY d.uploadedAt DESC
    LIMIT $limit
  `;

  try {
    const result = await client.query(cypher, { limit });

    if (!result.data?.values?.length) {
      return [];
    }

    const fields = result.data.fields;
    const docIndex = fields.indexOf('doc');
    return result.data.values.map((row) => row[docIndex] as DocumentNode);
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to get documents needing review: ${message}`);
  }
}

/**
 * Update document after manual correction.
 *
 * @param client - Neo4j HTTP client
 * @param documentId - Document UUID
 * @param updates - Fields to update
 */
export async function updateDocumentReviewStatus(
  client: Neo4jHTTPClient,
  documentId: string,
  updates: {
    needsReview?: boolean;
    extractionConfidence?: number;
  }
): Promise<void> {
  const setClauses: string[] = [];
  const params: Record<string, unknown> = { id: documentId };

  if (updates.needsReview !== undefined) {
    setClauses.push('d.needsReview = $needsReview');
    params.needsReview = updates.needsReview;
  }

  if (updates.extractionConfidence !== undefined) {
    setClauses.push('d.extractionConfidence = $confidence');
    params.confidence = updates.extractionConfidence;
  }

  if (setClauses.length === 0) {
    return;
  }

  const cypher = `
    MATCH (d:Document {id: $id})
    SET ${setClauses.join(', ')}, d.updatedAt = datetime()
  `;

  try {
    await client.query(cypher, params);
  } catch (error) {
    const message = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to update document: ${message}`);
  }
}
```
  </action>
  <verify>
TypeScript compiles:
```
cd apps/omnii_mcp && bun build src/ingestion/sources/files/graph-operations.ts --outdir /tmp --target node
```
  </verify>
  <done>
graph-operations.ts exports createDocumentWithChunks, checkDuplicateDocument, getDocumentById, getDocumentsNeedingReview.
Chunks created with embeddings and linked via HAS_CHUNK and NEXT_CHUNK relationships.
File compiles successfully.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. Document constraints added to constraints.ts
2. checkDuplicateDocument() queries by fileHash
3. createDocumentWithChunks() creates Document, Chunks with embeddings, and relationships
4. HAS_CHUNK relationships link Document to Chunks
5. NEXT_CHUNK relationships link sequential chunks
6. getDocumentsNeedingReview() returns low-confidence documents
7. All files compile without TypeScript errors
</verification>

<success_criteria>
- Document uniqueness constraint on fileHash prevents duplicates
- createDocumentWithChunks() generates embeddings using existing service
- Chunks stored with position, text, embedding, documentId
- HAS_CHUNK and NEXT_CHUNK relationships created correctly
- getDocumentsNeedingReview() returns documents with needsReview=true
- updateDocumentReviewStatus() allows marking reviewed documents
</success_criteria>

<output>
After completion, create `.planning/phases/08-file-ingestion-pipeline/08-04-SUMMARY.md`
</output>
